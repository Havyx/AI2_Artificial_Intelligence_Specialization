{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já sabemos fazer o processamento inicial no texto e construir um classificador, vamos explorar as arquiteturas que permitam processar a sequência de texto mais efetivamente.\n",
    "[GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "[LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "\n",
    "Como estes algoritmos são capazes de processar sequências de caracteres, podemos considerar tarefas mais desafiadoras.\n",
    "\n",
    "Considere o problema de **Gerar Texto**.\n",
    "\n",
    "Na aula passada consideramos a tarefa de classificar o \"sentimento\" da sentenca. Agora vamos resolver um problema mais desafiador. \n",
    "\n",
    "Nesta aula vamos modelar o estilo de \"William Shakespeare\", treinando um modelo para que complete poemas \"como Shakespeare os escreveria\".\n",
    "\n",
    "Basicamente, o problema consiste em prever a próxima palavra dado o restante da frase, por exemplo:\n",
    "\n",
    "**x** *= \"That thereby beauty's rose might never\"*, **y** *= \"die\"*\n",
    "\n",
    "\n",
    "**x** *= \"Or who is he so fond will be the\"*, **y** *= \"tomb\"*\n",
    "\n",
    "Para tanto, preparamos um dataset com exemplos de sonetos escritos por Shakespeare:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from fairest creatures we desire increase,', \"that thereby beauty's rose might never die,\", 'but as the riper should by time decease,', 'his tender heir might bear his memory:', 'but thou, contracted to thine own bright eyes,']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "data = open('./data/text_generation/sonnets.txt').read()\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "print(corpus[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como de costume, precisamos modelar esse problema de maneira que ele seja tratável com uma rede neural.\n",
    "\n",
    "Agora já sabemos utilizar a classe [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) para codificar strings, porém no problema anterior tínhamos que fazer uma classificacão binária de um texto de tamanho máximo constante.\n",
    "\n",
    "Agora, o modelo deve responder com uma palavra, como podemos modificar nossa rede neural para resolver este problema?\n",
    "\n",
    "Antes de modificar a rede neural, vamos pensar como transformaremos o texto bruto no dataset para treinamento do modelo.\n",
    "\n",
    "Uma forma é se aproveitar da funcão [pad_sequence](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) com *padding='pre'* para que normalizemos o tamanho da string de forma que a \"última palavra\" (isto é, a palavra a ser predita pelo modelo) fique na última posicão e seja fácil de se extrair.\n",
    "\n",
    "por exemplo:\n",
    "\n",
    "\n",
    "`\n",
    "\"from fairest creatures we desire increase\" -> (Tokenizer) -> \n",
    "[1, 2, 3, 4, 5, 6] -> (padding) -> [0, 0, 1, 2, 3, 4, 5, 6]`\n",
    "\n",
    "Assim, *y* é facilmente acessível no índice `sentenca[-1]`.\n",
    "\n",
    "Portanto, o primeiro passo é utilizar o tokenizer para que o vocabulário seja definido:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[34, 417, 877, 166, 213, 517],\n",
       " [8, 878, 134, 351, 102, 156, 199],\n",
       " [16, 22, 2, 879, 61, 30, 48, 634],\n",
       " [25, 311, 635, 102, 200, 25, 278],\n",
       " [16, 10, 880, 3, 62, 85, 214, 53]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_vocab_size = 1000\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, lower=True)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "corpus_seqs = tokenizer.texts_to_sequences(corpus)\n",
    "corpus_seqs[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é organizar nossa base de treinamento. \n",
    "\n",
    "Como queremos que o modelo seja capaz de prever a próxima palavra em qualquer ponto da frase, vamos considerar todas as subfrases possíveis de se formar com cada linha de soneto.\n",
    "\n",
    "Por exemplo, para a frase `\"from fairest creatures we desire increase\"` as seguintes subfrases serão geradas\n",
    "\n",
    "`\n",
    "\"from fairest\"\n",
    "\"from fairest creatures\"\n",
    "\"from fairest creatures we\"\n",
    "\"from fairest creatures we desire\"\n",
    "\"from fairest creatures we desire increase\"\n",
    "`\n",
    "\n",
    "Ou, em sua versão codificada:\n",
    "\n",
    "`\n",
    "[35, 418]\n",
    "[35, 418, 878]\n",
    "[35, 418, 878, 167]\n",
    "[35, 418, 878, 167, 214]\n",
    "[35, 418, 878, 167, 214, 518]\n",
    "`\n",
    "\n",
    "Codifique uma funcao `process_corpus(seqs)` que vai transformar o corpus_seqs definido na célula acima criando as subfrases como descrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34, 417], [34, 417, 877], [34, 417, 877, 166], [34, 417, 877, 166, 213], [34, 417, 877, 166, 213, 517], [8, 878], [8, 878, 134], [8, 878, 134, 351], [8, 878, 134, 351, 102], [8, 878, 134, 351, 102, 156]]\n"
     ]
    }
   ],
   "source": [
    "def process_corpus(seqs):\n",
    "    res_seqs = []\n",
    "    for seq in seqs:\n",
    "        if len(seq) > 1:\n",
    "            for i in range(1, len(seq)):\n",
    "                res_seqs.append(seq[0:i+1])\n",
    "    return res_seqs\n",
    "\n",
    "proc_corpus = process_corpus(corpus_seqs)\n",
    "print(proc_corpus[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos fazer o processamento final das sentencas com [padding](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences).\n",
    "\n",
    "\n",
    "O faremos de modo que a \"palavra a ser predita\" fique no último índice, facilitando o processamento posterior quando precisarmos separar a base em \"x\" e \"y\".\n",
    "\n",
    "Note que aqui também devemos determinar o \"tamanho máximo da frase\", que efetivamente vai determinar até quantas palavras no passado o modelo vai olhar para definir a próxima palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0  34 417]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0  34 417 877]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0  34 417 877 166]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  34 417 877 166 213]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  34 417 877 166 213 517]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 30\n",
    "\n",
    "\n",
    "padded_dataset = pad_sequences(proc_corpus, maxlen=max_len + 1, padding='pre', truncating='pre')\n",
    "print(padded_dataset[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos os dados organizados de uma maneira mais palatável para uma rede neural.\n",
    "\n",
    "Diferentemente da rede construída para a tarefa de classificacão de sentimentos, aqui nossa rede neural precisa ter `max_vocab_size` saídas, que efetivamente significa que a saída da rede vai ser uma probabilidade de a próxima palavra ser cada uma das possiblidades em nosso vocábulo.\n",
    "\n",
    "Para o treinamento, é mais indicado que codifiquemos a palavra a ser prevista com um one-hot encoding, assim a saída da rede vai ser uma probabilidade para cada palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0  34]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0  34 417]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np\n",
    "\n",
    "X = np.array(padded_dataset[:,:-1])\n",
    "y = padded_dataset[:,-1]\n",
    "y = ku.to_categorical(y, num_classes = max_vocab_size)\n",
    "\n",
    "\n",
    "print(X[0:2])\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim, o processamento do texto está terminado e podemos treinar nossa rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 100)           100000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               25500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              501000    \n",
      "=================================================================\n",
      "Total params: 737,100\n",
      "Trainable params: 737,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Dropout, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(max_vocab_size/2, activation='relu'))\n",
    "model.add(Dense(max_vocab_size, activation='softmax'))\n",
    "# Pick an optimizer\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12880 samples\n",
      "Epoch 1/100\n",
      "12880/12880 [==============================] - 12s 929us/sample - loss: 5.9763 - accuracy: 0.0248\n",
      "Epoch 2/100\n",
      "12880/12880 [==============================] - 9s 680us/sample - loss: 5.7770 - accuracy: 0.0293\n",
      "Epoch 3/100\n",
      "12880/12880 [==============================] - 9s 680us/sample - loss: 5.6866 - accuracy: 0.0331\n",
      "Epoch 4/100\n",
      "12880/12880 [==============================] - 9s 692us/sample - loss: 5.6102 - accuracy: 0.0380\n",
      "Epoch 5/100\n",
      "12880/12880 [==============================] - 9s 708us/sample - loss: 5.5583 - accuracy: 0.0410\n",
      "Epoch 6/100\n",
      "12880/12880 [==============================] - 9s 724us/sample - loss: 5.5039 - accuracy: 0.0408\n",
      "Epoch 7/100\n",
      "12880/12880 [==============================] - 9s 722us/sample - loss: 5.4448 - accuracy: 0.0433\n",
      "Epoch 8/100\n",
      "12880/12880 [==============================] - 10s 749us/sample - loss: 5.3789 - accuracy: 0.0467\n",
      "Epoch 9/100\n",
      "12880/12880 [==============================] - 10s 772us/sample - loss: 5.2997 - accuracy: 0.0502\n",
      "Epoch 10/100\n",
      "12880/12880 [==============================] - 9s 737us/sample - loss: 5.2074 - accuracy: 0.0556\n",
      "Epoch 11/100\n",
      "12880/12880 [==============================] - 10s 777us/sample - loss: 5.1168 - accuracy: 0.0621\n",
      "Epoch 12/100\n",
      "12880/12880 [==============================] - 10s 793us/sample - loss: 5.0271 - accuracy: 0.0636\n",
      "Epoch 13/100\n",
      "12880/12880 [==============================] - 10s 792us/sample - loss: 4.9371 - accuracy: 0.0678\n",
      "Epoch 14/100\n",
      "12880/12880 [==============================] - 10s 815us/sample - loss: 4.8464 - accuracy: 0.0725\n",
      "Epoch 15/100\n",
      "12880/12880 [==============================] - 10s 811us/sample - loss: 4.7534 - accuracy: 0.0774\n",
      "Epoch 16/100\n",
      "12880/12880 [==============================] - 10s 805us/sample - loss: 4.6600 - accuracy: 0.0799\n",
      "Epoch 17/100\n",
      "12880/12880 [==============================] - 10s 803us/sample - loss: 4.5613 - accuracy: 0.0852\n",
      "Epoch 18/100\n",
      "12880/12880 [==============================] - 10s 806us/sample - loss: 4.4601 - accuracy: 0.0883\n",
      "Epoch 19/100\n",
      "12880/12880 [==============================] - 13s 1ms/sample - loss: 4.3595 - accuracy: 0.0946\n",
      "Epoch 20/100\n",
      "12880/12880 [==============================] - 15s 1ms/sample - loss: 4.2609 - accuracy: 0.1021\n",
      "Epoch 21/100\n",
      "12880/12880 [==============================] - 14s 1ms/sample - loss: 4.1545 - accuracy: 0.1066\n",
      "Epoch 22/100\n",
      "12880/12880 [==============================] - 15s 1ms/sample - loss: 4.0445 - accuracy: 0.1158\n",
      "Epoch 23/100\n",
      "12880/12880 [==============================] - 14s 1ms/sample - loss: 3.9338 - accuracy: 0.1308\n",
      "Epoch 24/100\n",
      "12880/12880 [==============================] - 12s 964us/sample - loss: 3.8259 - accuracy: 0.1431\n",
      "Epoch 25/100\n",
      "12880/12880 [==============================] - 12s 911us/sample - loss: 3.7193 - accuracy: 0.1539\n",
      "Epoch 26/100\n",
      "12880/12880 [==============================] - 11s 866us/sample - loss: 3.6178 - accuracy: 0.1688\n",
      "Epoch 27/100\n",
      "12880/12880 [==============================] - 11s 857us/sample - loss: 3.5110 - accuracy: 0.1851\n",
      "Epoch 28/100\n",
      "12880/12880 [==============================] - 10s 792us/sample - loss: 3.4150 - accuracy: 0.2013\n",
      "Epoch 29/100\n",
      "12880/12880 [==============================] - 10s 802us/sample - loss: 3.3158 - accuracy: 0.2193\n",
      "Epoch 30/100\n",
      "12880/12880 [==============================] - 10s 782us/sample - loss: 3.2240 - accuracy: 0.2358\n",
      "Epoch 31/100\n",
      "12880/12880 [==============================] - 10s 795us/sample - loss: 3.1313 - accuracy: 0.2536\n",
      "Epoch 32/100\n",
      "12880/12880 [==============================] - 10s 790us/sample - loss: 3.0468 - accuracy: 0.2676\n",
      "Epoch 33/100\n",
      "12880/12880 [==============================] - 12s 925us/sample - loss: 2.9607 - accuracy: 0.2871\n",
      "Epoch 34/100\n",
      "12880/12880 [==============================] - 14s 1ms/sample - loss: 2.8872 - accuracy: 0.3002\n",
      "Epoch 35/100\n",
      "12880/12880 [==============================] - 14s 1ms/sample - loss: 2.8156 - accuracy: 0.3186\n",
      "Epoch 36/100\n",
      "12880/12880 [==============================] - 14s 1ms/sample - loss: 2.7410 - accuracy: 0.3280\n",
      "Epoch 37/100\n",
      "12880/12880 [==============================] - 14s 1ms/sample - loss: 2.6677 - accuracy: 0.3457\n",
      "Epoch 38/100\n",
      "12880/12880 [==============================] - 13s 1ms/sample - loss: 2.6064 - accuracy: 0.3599\n",
      "Epoch 39/100\n",
      "12880/12880 [==============================] - 12s 969us/sample - loss: 2.5379 - accuracy: 0.3726\n",
      "Epoch 40/100\n",
      "12880/12880 [==============================] - 12s 926us/sample - loss: 2.4774 - accuracy: 0.3901\n",
      "Epoch 41/100\n",
      "12880/12880 [==============================] - 11s 888us/sample - loss: 2.4190 - accuracy: 0.4000\n",
      "Epoch 42/100\n",
      " 6272/12880 [=============>................] - ETA: 5s - loss: 2.2840 - accuracy: 0.4297"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo pode agora ser utilizado para  prever as próximas palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_word(model, frase):\n",
    "    sequences = tokenizer.texts_to_sequences([frase])\n",
    "    padded = pad_sequences(sequences, maxlen=max_len , padding='pre', truncating='pre')\n",
    "    predicted = model.predict_classes(padded)\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    return output_word\n",
    "    \n",
    "get_next_word(model, 'please, save me in this')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inclusive, podemos gerar uma sequencia maior de palavras para formar um novo soneto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_words = 50\n",
    "\n",
    "sentence = 'please, save me in this'\n",
    "for i in range(generate_words):\n",
    "    sentence = sentence + \" \" + get_next_word(model, sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uma determinada entrada, o modelo sempre vai responder a mesma palavra, o que não é interessante. Modifique a funcão `get_next_word` para retornar uma palavra com probabilidade igual à especificada pelo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste diversas possibilidades de arquitetura, com modelos bidirecionais, LSTM e GRUs, quais deles obtém um resultado melhor?\n",
    "\n",
    "\n",
    "Após isso, tente treinar um modelo semelhante usando poemas de [Goncalves Dias](http://www.dominiopublico.gov.br/download/texto/bv000115.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
