{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante essa aula vamos explorar um pouco os tipos de tarefas que podemos resolver utilizando os conceitos aprendidos nas últimas 2 aulas.\n",
    "\n",
    "A primeira tarefa será de **classificacão de texto**, similar à tarefa explorada na primeira semana.\n",
    "\n",
    "A tarefa consiste em analisar uma notícia e classificá-la dentre um conjunto possível de temas, por exemplo, *tecnologia*, *esporte*, etc.\n",
    "\n",
    "Para isto, vamos utilizar o dataset liberado pela [BBC](http://mlg.ucd.ie/datasets/bbc.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/text_classification/bbc-text.csv\")\n",
    "\n",
    "num_classes = len(data.category.value_counts())\n",
    "\n",
    "\n",
    "training_proportion = 0.9\n",
    "\n",
    "training_index = int(training_proportion * len(data))\n",
    "\n",
    "train_data = data.iloc[:training_index,:]\n",
    "test_data = data.iloc[training_index:, :]\n",
    "\n",
    "train_text = train_data.loc[:, 'text'].tolist()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas últimas aulas, apesar de utilizarmos embeddings em todas as tarefas, estávamos os aprendendo do zero baseando-se na base de treinamento.\n",
    "\n",
    "Na verdade, não há necessidade disso porque já existe uma variedade muito grande de embeddings pré-treinados que podem ser utilizados.\n",
    "\n",
    "Para este exercício vamos utilizar o [GloVe](https://nlp.stanford.edu/projects/glove/), mas existem muitas opcões de embeddings pré-treinados, inclusive em português.\n",
    "\n",
    "Antes de rodar as próximas linhas, baixe o arquivo [glove.6B.50d.txt](https://github.com/uclnlp/inferbeddings/blob/master/data/glove/glove.6B.50d.txt.gz)\n",
    "\n",
    "Vamos ler o conteúdo do arquivo com embeddings pré-treinados, para posteriormente utilizá-los na nossa camada de Embedding. O propósito da próxima célula é determinar o vocabulário para a nossa base de treinamento, e carregar os embeddings para todas as palavras encontradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1995 word vectors.\n",
      "[[ 0.88387001 -0.14199001  0.13565999  0.098682    0.51217997  0.49138001\n",
      "  -0.47154999 -0.30741999  0.01963     0.12685999  0.073524    0.35835999\n",
      "  -0.60873997 -0.18675999  0.78934997  0.54534     0.1106     -0.29229999\n",
      "   0.059041   -0.69551003 -0.18804     0.19454999  0.32269001 -0.49981001\n",
      "   0.30599999 -2.3901999  -0.60749     0.37107     0.078912   -0.23896\n",
      "   3.83899999 -0.20355    -0.35613    -0.69185001 -0.17497    -0.35323\n",
      "   0.10598    -0.039303    0.015701    0.038279   -0.35282999  0.44881999\n",
      "  -0.16534001  0.31579     0.14963    -0.071277   -0.53505999  0.52710998\n",
      "  -0.20148     0.0095952 ]\n",
      " [ 0.61183    -0.22071999 -0.10898    -0.052967    0.50804001  0.34683999\n",
      "  -0.33557999 -0.19152001 -0.035865    0.1051      0.07935     0.2449\n",
      "  -0.4373     -0.33344001  0.57479     0.69051999  0.29712999  0.090669\n",
      "  -0.54992002 -0.46176001  0.10113    -0.02024     0.28479001  0.043512\n",
      "   0.45734999 -2.0466001  -0.58083999  0.61796999  0.65179998 -0.58262998\n",
      "   4.07859993 -0.25420001 -0.14648999 -0.34321001 -0.25437    -0.44677001\n",
      "   0.12657     0.28134     0.13331001 -0.36974001  0.050059   -0.10058\n",
      "  -0.017907    0.11142    -0.71798003  0.491      -0.099974   -0.043688\n",
      "  -0.097922    0.16806   ]\n",
      " [ 0.30045     0.25005999 -0.16692001  0.19230001  0.026921   -0.079486\n",
      "  -0.91382998 -0.1974     -0.053413   -0.40845999 -0.26844001 -0.28211999\n",
      "  -0.5         0.1221      0.39030001  0.17797001 -0.4429     -0.40478\n",
      "  -0.95050001 -0.16897     0.77793002  0.33524999  0.3346     -0.1754\n",
      "  -0.12017    -1.78610003  0.29240999  0.55932999  0.029982   -0.32416999\n",
      "   3.9296999   0.1088     -0.57335001 -0.17842001  0.0041748  -0.16309001\n",
      "   0.45076999 -0.16123    -0.17310999 -0.087889   -0.089032    0.062001\n",
      "  -0.19946    -0.38863    -0.18232     0.060751    0.098603   -0.07131\n",
      "   0.23052    -0.51938999]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "max_vocab = 2000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_vocab)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "indexes = min(max_vocab, len(tokenizer.word_index))\n",
    "\n",
    "num_emb = 0\n",
    "cons_keys = []\n",
    "for word in tokenizer.word_index.keys():\n",
    "    cons_keys.append(word)\n",
    "    num_emb += 1\n",
    "    if num_emb == max_vocab:\n",
    "        break\n",
    "\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('data/text_classification/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in cons_keys:\n",
    "        coefs = np.array(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((max_vocab, 50))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i>=max_vocab:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix[10:13])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora faca o processamento restante necessário, including padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "sentence_size = 500\n",
    "\n",
    "dataset_train_sequences = tokenizer.texts_to_sequences(train_data.loc[:,'text'])\n",
    "dataset_test_sequences = tokenizer.texts_to_sequences(test_data.loc[:,'text'])\n",
    "\n",
    "padded_train = pad_sequences(dataset_train_sequences, maxlen=sentence_size, padding='post', truncating='post')\n",
    "padded_test = pad_sequences(dataset_test_sequences, maxlen=sentence_size, padding='post', truncating='post')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 50)           100000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               58880     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              129000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 5005      \n",
      "=================================================================\n",
      "Total params: 292,885\n",
      "Trainable params: 192,885\n",
      "Non-trainable params: 100,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# 50 é o número de dimensoes do embedding (definido pelo arquivo que baixamos)\n",
    "#trainable = False significa que esses pesos nao sao atualizados durante o treinamento.\n",
    "model.add(Embedding(max_vocab, 50, weights=[embedding_matrix], input_length=sentence_size, trainable=False))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(max_vocab/2, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.utils as ku\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "labelencoder.fit_transform(train_data.loc[:, 'category'])\n",
    "\n",
    "label_train = train_data.loc[:, ['category']].apply(LabelEncoder().fit_transform).values\n",
    "label_test = test_data.loc[:, ['category']].apply(LabelEncoder().fit_transform).values\n",
    "\n",
    "label_train = ku.to_categorical(label_train, num_classes = num_classes)\n",
    "label_test = ku.to_categorical(label_test, num_classes = num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2002 samples, validate on 223 samples\n",
      "Epoch 1/10\n",
      "2002/2002 - 17s - loss: 0.7438 - accuracy: 0.7398 - val_loss: 0.3838 - val_accuracy: 0.8610\n",
      "Epoch 2/10\n",
      "2002/2002 - 2s - loss: 0.2770 - accuracy: 0.9116 - val_loss: 0.2811 - val_accuracy: 0.9193\n",
      "Epoch 3/10\n",
      "2002/2002 - 2s - loss: 0.2461 - accuracy: 0.9221 - val_loss: 0.3172 - val_accuracy: 0.9013\n",
      "Epoch 4/10\n",
      "2002/2002 - 2s - loss: 0.1748 - accuracy: 0.9461 - val_loss: 0.2820 - val_accuracy: 0.9103\n",
      "Epoch 5/10\n",
      "2002/2002 - 2s - loss: 0.1547 - accuracy: 0.9476 - val_loss: 0.2689 - val_accuracy: 0.9193\n",
      "Epoch 6/10\n",
      "2002/2002 - 3s - loss: 0.1071 - accuracy: 0.9620 - val_loss: 0.3902 - val_accuracy: 0.9058\n",
      "Epoch 7/10\n",
      "2002/2002 - 3s - loss: 0.1146 - accuracy: 0.9565 - val_loss: 0.3175 - val_accuracy: 0.9193\n",
      "Epoch 8/10\n",
      "2002/2002 - 3s - loss: 0.0892 - accuracy: 0.9715 - val_loss: 0.3330 - val_accuracy: 0.9327\n",
      "Epoch 9/10\n",
      "2002/2002 - 3s - loss: 0.0704 - accuracy: 0.9760 - val_loss: 0.2688 - val_accuracy: 0.9372\n",
      "Epoch 10/10\n",
      "2002/2002 - 3s - loss: 0.0700 - accuracy: 0.9725 - val_loss: 0.3024 - val_accuracy: 0.9193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7bd01b3c90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded_train,label_train, epochs = num_epochs, validation_data=(padded_test,label_test), verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'business', 'sport'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_topic(text):\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    proc_text = pad_sequences(seq, maxlen=sentence_size, padding='post', truncating='post')\n",
    "    topic = np.argmax(model.predict(proc_text), axis=1)\n",
    "    topic = labelencoder.inverse_transform(topic)\n",
    "    return topic\n",
    "\n",
    "predict_topic(['A lot of data centers',\"A lot of stock prices and financial data\", 'A lot of goals scored'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já somos experts em classificar sentimentos em textos, vamos considerar uma tarefa diferente mas igualmente importante: **Traducão**.\n",
    "\n",
    "Como realizar traducões entre duas linguas requer modelos grandes e um grande corpus de treinamento, vamos considerar uma tarefa de \"traducão\" mais simples.\n",
    "\n",
    "A tarefa que consideraremos é a traducão de uma data em texto livre para um formato mais palatável para o computador. Alguns exemplos de traducão estão abaixo:\n",
    "\n",
    "`9 may 1998 -> 1998-05-09\n",
    "10.11.19 -> 2019-11-10\n",
    "9/10/70 -> 1970-09-10\n",
    "saturday april 28 1990 -> 1990-04-28\n",
    "thursday january 26 1995 -> 1995-01-26\n",
    "monday march 7 1983 -> 1983-03-07`\n",
    "\n",
    "Para realizar esta tarefa, modelaremos o problema de uma forma um pouco diferente. Para a criacão de nosso vocabulário, designaremos um código para cada caractere (não cada palavra), formando um vocabulario de origem e um de destino.\n",
    "\n",
    "Seu modelo irá ler um conjunto de caracteres de entrada de tamanho `n`, processá-lo e dar como saída um conjunto de caracteres de tamanho `10`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10, 12)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/translation/dates_dataset.csv', header=None)\n",
    "train_index = int(0.9 * len(data))\n",
    "train_data = data[:train_index]\n",
    "test_data = data[train_index:]\n",
    "\n",
    "tok_origem = Tokenizer(char_level=True)\n",
    "tok_destino = Tokenizer(char_level = True)\n",
    "\n",
    "tok_origem.fit_on_texts(train_data.loc[:,0])\n",
    "tok_destino.fit_on_texts(train_data.loc[:,1])\n",
    "\n",
    "seq_train = tok_origem.texts_to_sequences(train_data.loc[:,0])\n",
    "seq_test = tok_origem.texts_to_sequences(test_data.loc[:,0])\n",
    "\n",
    "label_train = np.array(tok_destino.texts_to_sequences(train_data.loc[:,1]))\n",
    "label_test = np.array(tok_destino.texts_to_sequences(test_data.loc[:,1]))\n",
    "\n",
    "label_train = ku.to_categorical(label_train, num_classes = len(tok_destino.word_index)+1)\n",
    "label_test = ku.to_categorical(label_test, num_classes = len(tok_destino.word_index)+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_orig = 30\n",
    "max_size_dest = 10\n",
    "\n",
    "padded_train = pad_sequences(seq_train, maxlen=max_size_orig, padding='post', truncating='post')\n",
    "padded_test = pad_sequences(seq_test, maxlen=max_size_orig, padding='post', truncating='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 30, 50)            1800      \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 120)               15480     \n",
      "_________________________________________________________________\n",
      "reshape_14 (Reshape)         (None, 10, 12)            0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 10, 12)            156       \n",
      "=================================================================\n",
      "Total params: 55,196\n",
      "Trainable params: 55,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Reshape, Lambda, Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tok_origem.word_index)+1, 50,  input_length=max_size_orig))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(max_size_dest*(len(tok_destino.word_index)+1), activation='sigmoid'))\n",
    "model.add(Reshape((max_size_dest,len(tok_destino.word_index)+1)))\n",
    "model.add(Dense(len(tok_destino.word_index)+1, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "9000/9000 - 2s - loss: 1.5737 - accuracy: 0.4613 - val_loss: 1.3637 - val_accuracy: 0.4933\n",
      "Epoch 2/50\n",
      "9000/9000 - 2s - loss: 1.3011 - accuracy: 0.4853 - val_loss: 1.2577 - val_accuracy: 0.4889\n",
      "Epoch 3/50\n",
      "9000/9000 - 2s - loss: 1.2389 - accuracy: 0.4852 - val_loss: 1.2222 - val_accuracy: 0.4910\n",
      "Epoch 4/50\n",
      "9000/9000 - 2s - loss: 1.2129 - accuracy: 0.4864 - val_loss: 1.2043 - val_accuracy: 0.4920\n",
      "Epoch 5/50\n",
      "9000/9000 - 2s - loss: 1.1991 - accuracy: 0.4868 - val_loss: 1.1933 - val_accuracy: 0.4933\n",
      "Epoch 6/50\n",
      "9000/9000 - 2s - loss: 1.1911 - accuracy: 0.4863 - val_loss: 1.1867 - val_accuracy: 0.4938\n",
      "Epoch 7/50\n",
      "9000/9000 - 2s - loss: 1.1855 - accuracy: 0.4865 - val_loss: 1.1831 - val_accuracy: 0.4916\n",
      "Epoch 8/50\n",
      "9000/9000 - 1s - loss: 1.1814 - accuracy: 0.4857 - val_loss: 1.1803 - val_accuracy: 0.4920\n",
      "Epoch 9/50\n",
      "9000/9000 - 2s - loss: 1.1786 - accuracy: 0.4864 - val_loss: 1.1773 - val_accuracy: 0.4898\n",
      "Epoch 10/50\n",
      "9000/9000 - 2s - loss: 1.1766 - accuracy: 0.4857 - val_loss: 1.1752 - val_accuracy: 0.4898\n",
      "Epoch 11/50\n",
      "9000/9000 - 2s - loss: 1.1748 - accuracy: 0.4864 - val_loss: 1.1736 - val_accuracy: 0.4935\n",
      "Epoch 12/50\n",
      "9000/9000 - 2s - loss: 1.1735 - accuracy: 0.4861 - val_loss: 1.1723 - val_accuracy: 0.4932\n",
      "Epoch 13/50\n",
      "9000/9000 - 1s - loss: 1.1724 - accuracy: 0.4873 - val_loss: 1.1720 - val_accuracy: 0.4903\n",
      "Epoch 14/50\n",
      "9000/9000 - 1s - loss: 1.1708 - accuracy: 0.4863 - val_loss: 1.1475 - val_accuracy: 0.5021\n",
      "Epoch 15/50\n",
      "9000/9000 - 2s - loss: 1.1489 - accuracy: 0.4948 - val_loss: 1.1492 - val_accuracy: 0.5015\n",
      "Epoch 16/50\n",
      "9000/9000 - 2s - loss: 1.1496 - accuracy: 0.4953 - val_loss: 1.1508 - val_accuracy: 0.5007\n",
      "Epoch 17/50\n",
      "9000/9000 - 2s - loss: 1.1363 - accuracy: 0.5000 - val_loss: 1.1315 - val_accuracy: 0.5064\n",
      "Epoch 18/50\n",
      "9000/9000 - 2s - loss: 1.1009 - accuracy: 0.5134 - val_loss: 1.0880 - val_accuracy: 0.5339\n",
      "Epoch 19/50\n",
      "9000/9000 - 2s - loss: 1.0418 - accuracy: 0.5617 - val_loss: 0.9977 - val_accuracy: 0.5931\n",
      "Epoch 20/50\n",
      "9000/9000 - 1s - loss: 0.9419 - accuracy: 0.6177 - val_loss: 0.9121 - val_accuracy: 0.6285\n",
      "Epoch 21/50\n",
      "9000/9000 - 1s - loss: 0.8816 - accuracy: 0.6379 - val_loss: 0.8684 - val_accuracy: 0.6425\n",
      "Epoch 22/50\n",
      "9000/9000 - 1s - loss: 0.8552 - accuracy: 0.6422 - val_loss: 0.8322 - val_accuracy: 0.6463\n",
      "Epoch 23/50\n",
      "9000/9000 - 2s - loss: 0.8089 - accuracy: 0.6569 - val_loss: 0.7973 - val_accuracy: 0.6606\n",
      "Epoch 24/50\n",
      "9000/9000 - 2s - loss: 0.7678 - accuracy: 0.6724 - val_loss: 0.7512 - val_accuracy: 0.6733\n",
      "Epoch 25/50\n",
      "9000/9000 - 2s - loss: 0.7230 - accuracy: 0.6893 - val_loss: 0.7076 - val_accuracy: 0.6938\n",
      "Epoch 26/50\n",
      "9000/9000 - 1s - loss: 0.6842 - accuracy: 0.7007 - val_loss: 0.6779 - val_accuracy: 0.7001\n",
      "Epoch 27/50\n",
      "9000/9000 - 2s - loss: 0.6459 - accuracy: 0.7134 - val_loss: 0.6274 - val_accuracy: 0.7169\n",
      "Epoch 28/50\n",
      "9000/9000 - 2s - loss: 0.6105 - accuracy: 0.7253 - val_loss: 0.5986 - val_accuracy: 0.7272\n",
      "Epoch 29/50\n",
      "9000/9000 - 2s - loss: 0.5701 - accuracy: 0.7422 - val_loss: 0.5473 - val_accuracy: 0.7475\n",
      "Epoch 30/50\n",
      "9000/9000 - 2s - loss: 0.5137 - accuracy: 0.7580 - val_loss: 0.4770 - val_accuracy: 0.7757\n",
      "Epoch 31/50\n",
      "9000/9000 - 1s - loss: 0.4686 - accuracy: 0.7766 - val_loss: 0.4477 - val_accuracy: 0.7836\n",
      "Epoch 32/50\n",
      "9000/9000 - 2s - loss: 0.4328 - accuracy: 0.7928 - val_loss: 0.4216 - val_accuracy: 0.8015\n",
      "Epoch 33/50\n",
      "9000/9000 - 2s - loss: 0.4088 - accuracy: 0.8087 - val_loss: 0.4000 - val_accuracy: 0.8188\n",
      "Epoch 34/50\n",
      "9000/9000 - 2s - loss: 0.3866 - accuracy: 0.8235 - val_loss: 0.3751 - val_accuracy: 0.8297\n",
      "Epoch 35/50\n",
      "9000/9000 - 1s - loss: 0.3484 - accuracy: 0.8407 - val_loss: 0.3352 - val_accuracy: 0.8453\n",
      "Epoch 36/50\n",
      "9000/9000 - 2s - loss: 0.3213 - accuracy: 0.8510 - val_loss: 0.3161 - val_accuracy: 0.8515\n",
      "Epoch 37/50\n",
      "9000/9000 - 2s - loss: 0.2974 - accuracy: 0.8618 - val_loss: 0.2963 - val_accuracy: 0.8621\n",
      "Epoch 38/50\n",
      "9000/9000 - 2s - loss: 0.2779 - accuracy: 0.8753 - val_loss: 0.2710 - val_accuracy: 0.8808\n",
      "Epoch 39/50\n",
      "9000/9000 - 1s - loss: 0.2596 - accuracy: 0.8850 - val_loss: 0.2654 - val_accuracy: 0.8770\n",
      "Epoch 40/50\n",
      "9000/9000 - 2s - loss: 0.2491 - accuracy: 0.8873 - val_loss: 0.2412 - val_accuracy: 0.8847\n",
      "Epoch 41/50\n",
      "9000/9000 - 1s - loss: 0.2331 - accuracy: 0.8908 - val_loss: 0.2363 - val_accuracy: 0.8877\n",
      "Epoch 42/50\n",
      "9000/9000 - 2s - loss: 0.2281 - accuracy: 0.8908 - val_loss: 0.2345 - val_accuracy: 0.8849\n",
      "Epoch 43/50\n",
      "9000/9000 - 2s - loss: 0.2179 - accuracy: 0.8928 - val_loss: 0.2164 - val_accuracy: 0.8930\n",
      "Epoch 44/50\n",
      "9000/9000 - 2s - loss: 0.2083 - accuracy: 0.8962 - val_loss: 0.2098 - val_accuracy: 0.8917\n",
      "Epoch 45/50\n",
      "9000/9000 - 2s - loss: 0.2028 - accuracy: 0.8999 - val_loss: 0.2037 - val_accuracy: 0.9074\n",
      "Epoch 46/50\n",
      "9000/9000 - 2s - loss: 0.1965 - accuracy: 0.9126 - val_loss: 0.1851 - val_accuracy: 0.9194\n",
      "Epoch 47/50\n",
      "9000/9000 - 2s - loss: 0.1790 - accuracy: 0.9184 - val_loss: 0.1782 - val_accuracy: 0.9201\n",
      "Epoch 48/50\n",
      "9000/9000 - 1s - loss: 0.1739 - accuracy: 0.9209 - val_loss: 0.1722 - val_accuracy: 0.9222\n",
      "Epoch 49/50\n",
      "9000/9000 - 1s - loss: 0.1653 - accuracy: 0.9277 - val_loss: 0.1576 - val_accuracy: 0.9325\n",
      "Epoch 50/50\n",
      "9000/9000 - 1s - loss: 0.1552 - accuracy: 0.9366 - val_loss: 0.1607 - val_accuracy: 0.9391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7a3bd2c1d0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "model.fit(padded_train,label_train, epochs = num_epochs, validation_data=(padded_test,label_test), verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 9 7 5 - 0 8 - 1 6', '1 9 9 1 - 0 7 - 1 2', '1 9 7 5 - 0 9 - 1 0']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_dates(dates):\n",
    "    seqs = tok_origem.texts_to_sequences(dates)\n",
    "    padded = pad_sequences(seqs, maxlen=max_size_orig, padding='post', truncating='post')\n",
    "    converted = np.argmax(model.predict(padded), axis=2)\n",
    "    converted = tok_destino.sequences_to_texts(converted)\n",
    "    return converted\n",
    "                           \n",
    "predict_dates(['august 26 1975', 'friday july 12 1991', '10 sep 1975'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o último problema a ser resolvido, vamos considerar um problema um pouco mais exótico, semelhante ao problema de geracão de palavras estudado na segunda aula.\n",
    "\n",
    "\n",
    "Para esta tarefa geraremos **nomes de Dinossauros**.\n",
    "\n",
    "Nomes de dinossauros têm uma certa semelhanca, por exemplo:\n",
    "\n",
    "`\n",
    "Abdallahsaurus\n",
    "Abelisaurus\n",
    "Abrictosaurus\n",
    "`\n",
    "\n",
    "Portanto, vamos treinar um modelo que completa o nome de um dinossauro!\n",
    "\n",
    "Para isto, novamente vamos gerar um token para cara caractere. O modelo deve ser treinado recebendo um nome parcial e o completando, por exemplo:\n",
    "\n",
    "`x = 'Abda', y = 'Abdallahsaurus'\n",
    "x = 'Abelisa', y = 'Abelisaurus'\n",
    "`\n",
    "\n",
    "O modelo pode ser treinado com a base de treinamento sendo gerada similarmente ao problema de gerar sonetos, mas adicionando um token \"\\n\" à saída para indicar o final do nome do Dinossauro:\n",
    "\n",
    "\n",
    "`x = 'A', y = 'Aardonyx\\n'\n",
    "x = 'Aa', y = 'Aardonyx\\n'\n",
    "x = 'Aar', y = 'Aardonyx\\n'\n",
    "...\n",
    "x = 'Aardony', y = 'Aardonyx\\n'\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aa</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aac</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aach</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aache</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aachen</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aacheno</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Aachenos</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Aachenosa</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aachenosau</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aachenosaur</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Aachenosauru</td>\n",
       "      <td>Aachenosaurus\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x                y\n",
       "0              A  Aachenosaurus\\n\n",
       "1             Aa  Aachenosaurus\\n\n",
       "2            Aac  Aachenosaurus\\n\n",
       "3           Aach  Aachenosaurus\\n\n",
       "4          Aache  Aachenosaurus\\n\n",
       "5         Aachen  Aachenosaurus\\n\n",
       "6        Aacheno  Aachenosaurus\\n\n",
       "7       Aachenos  Aachenosaurus\\n\n",
       "8      Aachenosa  Aachenosaurus\\n\n",
       "9     Aachenosau  Aachenosaurus\\n\n",
       "10   Aachenosaur  Aachenosaurus\\n\n",
       "11  Aachenosauru  Aachenosaurus\\n"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_list =[]\n",
    "largest_name = 0\n",
    "with open(\"data/text_generation/dinos.txt\", \"r\") as a_file:\n",
    "  for line in a_file:\n",
    "    largest_name = max(largest_name, len(line))\n",
    "    for i in range(len(line)-1):\n",
    "        row_list.append([line[:i+1], line])\n",
    "\n",
    "data = pd.DataFrame(row_list, columns = ['x', 'y'])\n",
    "data.head(12)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16536, 27, 28)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "training_index = int(0.9*len(data))\n",
    "train_data = data.loc[:training_index,:]\n",
    "test_data = data.loc[training_index:, :]\n",
    "\n",
    "tok = Tokenizer(char_level=True)\n",
    "tok.fit_on_texts(train_data.loc[:,'y'])\n",
    "words_num = len(tok.word_index)\n",
    "\n",
    "train_seq = tok.texts_to_sequences(train_data.loc[:,'x'])\n",
    "test_seq = tok.texts_to_sequences(test_data.loc[:,'x'])\n",
    "\n",
    "train_label_seq = tok.texts_to_sequences(train_data.loc[:,'y'])\n",
    "test_label_seq = tok.texts_to_sequences(test_data.loc[:,'y'])\n",
    "\n",
    "padded_train_seq = pad_sequences(train_seq, maxlen=largest_name, padding='post', truncating='post')\n",
    "padded_test_seq = pad_sequences(test_seq, maxlen=largest_name, padding='post', truncating='post')\n",
    "padded_label_train = pad_sequences(train_label_seq, maxlen=largest_name, padding='post', truncating='post')\n",
    "padded_label_test = pad_sequences(test_label_seq, maxlen=largest_name, padding='post', truncating='post')\n",
    "\n",
    "train_labels = ku.to_categorical(padded_label_train, num_classes =words_num+1)\n",
    "test_labels = ku.to_categorical(padded_label_test, num_classes =words_num+1)\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 27, 50)            1400      \n",
      "_________________________________________________________________\n",
      "bidirectional_19 (Bidirectio (None, 128)               58880     \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 756)               24948     \n",
      "_________________________________________________________________\n",
      "reshape_17 (Reshape)         (None, 27, 28)            0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 27, 28)            812       \n",
      "=================================================================\n",
      "Total params: 90,168\n",
      "Trainable params: 90,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(words_num+1, 50,  input_length=largest_name))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(largest_name*(words_num+1), activation='sigmoid'))\n",
    "model.add(Reshape((largest_name,words_num+1)))\n",
    "model.add(Dense(words_num+1, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16536 samples, validate on 1838 samples\n",
      "Epoch 1/5\n",
      "16536/16536 - 5s - loss: 0.0836 - accuracy: 0.9772 - val_loss: 0.0720 - val_accuracy: 0.9798\n",
      "Epoch 2/5\n",
      "16536/16536 - 4s - loss: 0.0721 - accuracy: 0.9796 - val_loss: 0.0713 - val_accuracy: 0.9798\n",
      "Epoch 3/5\n",
      "16536/16536 - 4s - loss: 0.0718 - accuracy: 0.9796 - val_loss: 0.0712 - val_accuracy: 0.9798\n",
      "Epoch 4/5\n",
      "16536/16536 - 4s - loss: 0.0714 - accuracy: 0.9797 - val_loss: 0.0702 - val_accuracy: 0.9798\n",
      "Epoch 5/5\n",
      "16536/16536 - 4s - loss: 0.0696 - accuracy: 0.9799 - val_loss: 0.0683 - val_accuracy: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f79c019c310>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model.fit(padded_train_seq,train_labels, epochs = num_epochs, validation_data=(padded_test_seq,test_labels), verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['testosaaauuu', 'brasiloaauuu', 'fiepoaaaauuu']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_dino(prefixos):\n",
    "    seqs = tok.texts_to_sequences(prefixos)\n",
    "    padded = pad_sequences(seqs, maxlen=largest_name, padding='post', truncating='post')\n",
    "    predictions = model.predict(padded)\n",
    "    predictions = np.argmax(predictions,axis=2)\n",
    "    converted = tok.sequences_to_texts(predictions)\n",
    "    converted = [\"\".join(x.split()) for x in converted]\n",
    "    final = [x.split('\\n')[0] for x in converted]\n",
    "    final = [prefixos[i]+final[i][len(prefixos[i]):] for i in range(len(final))]\n",
    "    return final\n",
    "             \n",
    "predict_dino(['test', 'brasil', 'fiep'])                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
