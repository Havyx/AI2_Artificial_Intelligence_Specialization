{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolução\n",
    "\n",
    "um diferencial da CNN é que eles assumem explicitamente que as entradas são imagens, o que nos permite codificar certas propriedades na arquitetura para reconhecer elementos específicos nas imagens.\n",
    "\n",
    "Fontes: \n",
    "* http://cs231n.github.io/convolutional-networks/\n",
    "* https://towardsdatascience.com/convolutional-neural-networks-for-beginners-practical-guide-with-python-and-keras-dc688ea90dca\n",
    "* https://towardsdatascience.com/basic-concepts-of-neural-networks-1a18a7aa2bd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processo de convolução\n",
    "\n",
    "* camadas densas aprendem padrões globais em seu espaço de features de entrada (por exemplo, para um dígito MNIST, padrões envolvendo todos os pixels)\n",
    "* camadas de convolução aprendem padrões locais: no caso de imagens, padrões encontrados em pequenas janelas 2D das entradas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Os padrões que aprendem são invariantes à translação. Depois de aprender um determinado padrão no canto inferior direito da imagem, uma convnet pode reconhecê-lo em qualquer lugar: por exemplo, no canto superior esquerdo. \n",
    "\n",
    "* Uma rede densamente conectada teria que aprender o padrão novamente se aparecesse em um novo local. Isso torna os dados de convolução eficientes no processamento de imagens (porque o mundo visual é fundamentalmente invariável na tradução): \n",
    "\n",
    "* eles precisam de menos amostras de treinamento para aprender representações que têm poder de generalização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CNNs aprendem hierarquias espaciais de padrões. \n",
    "* Uma primeira camada de convolução aprenderá pequenos padrões locais, como arestas, uma segunda camada de convolução aprenderá padrões maiores feitos dos recursos das primeiras camadas, e assim por diante. \n",
    "\n",
    "* Isso permite que os convnets aprendam com eficiência conceitos visuais cada vez mais complexos e abstratos (porque o mundo visual é fundamentalmente espacialmente hierárquico)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As convoluções operam sobre tensores 3D, chamados feature maps, com dois eixos espaciais (altura e largura) e um eixo de profundidade (também chamado de eixo de canais). \n",
    "* Para uma imagem RGB, a dimensão do eixo de profundidade é 3, porque a imagem possui três canais de cores: vermelho, verde e azul. \n",
    "\n",
    "* Para uma imagem em preto e branco, a profundidade é 1 (níveis de cinza). \n",
    "\n",
    "* A operação de convolução extrai patches de seu mapa de recursos de entrada e aplica a mesma transformação a todos esses patches, produzindo um mapa de recursos de saída. \n",
    "\n",
    "* Esse mapa de recursos de saída ainda é um tensor 3D: possui largura e altura. Sua profundidade pode ser arbitrária, porque a profundidade de saída é um parâmetro da camada e os diferentes canais nesse eixo de profundidade não representam mais cores específicas como na entrada RGB; em vez disso, eles representam filtros. \n",
    "\n",
    "* Os filtros codificam aspectos específicos dos dados de entrada: em um nível alto, um único filtro pode codificar o conceito \"presença de uma face na entrada\", por exemplo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As convoluções são definidas por dois parâmetros principais:\n",
    "\n",
    "* Tamanho das amostras extraídas das entradas - geralmente são 3 × 3 ou 5 × 5. (3 × 3 é uma escolha comum).\n",
    "* Profundidade do mapa de features de saída - O número de filtros calculados pela convolução. \n",
    "\n",
    "* Uma convolução funciona deslizando essas janelas de tamanho 3 × 3 ou 5 × 5 sobre o mapa de features de entrada 3D, parando em todos os locais possíveis e extraindo o patch 3D de recursos circundantes (forma (altura da janela, largura da janela, profundidade da entrada)). \n",
    "\n",
    "* Cada uma dessas formas 3D são transformadas (por meio de um produto tensorial com a mesma matriz de pesos aprendida, chamada de núcleo de convolução) em um vetor de forma 1D (profundidade_de_ saída). \n",
    "\n",
    "* Todos esses vetores são remontados espacialmente em um mapa de saída 3D da forma (altura, largura, profundidade_de_ saída). \n",
    "\n",
    "* Cada localização espacial no mapa de recursos de saída corresponde ao mesmo local no mapa de recursos de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efeitos de borda e padding (preenchimento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalmente após uma operação de convolução é comum ser realizada uma operação de pooling\n",
    "\n",
    "* as camadas de pool simplificam as informações coletadas pela camada convolucional e criam uma versão condensada das informações nelas contidas.\n",
    "  * Para uma janelas de pontos no espaço 2d é gerado um único ponto\n",
    "    * Valor máximo da janela (Maxpooling)\n",
    "    * Valor médio da janela (MaxAverage)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura Básica de uma rede convolucional\n",
    "* Utilizando camadas Conv e maxpool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "hangar-pil (0.3.2)               - PIL plugin for hangar\n",
      "Pillow-PIL (0.1dev)              - Pillow wrapper for PIL compatibility\n",
      "pil-compat (1.0.0)               - Compatibility modules, bridging PIL ->\n",
      "                                   Pillow\n",
      "segno-pil (0.1.6)                - PIL/Pillow plugin for the Segno (Micro) QR\n",
      "                                   Code generator\n",
      "SSIM-PIL (1.0.10)                - Comparison of two images using the\n",
      "                                   structural similarity algorithm (SSIM).\n",
      "                                   It's compatible with the PIL.\n",
      "PIL (1.1.6)                      - Python Imaging Library\n",
      "pidi-display-pil (0.1.0)         - pidi plugin for display output using PIL.\n",
      "Pil-Lite (0.1.1)                 - Python Imaging Library Lite\n",
      "m3-PIL (1.1.7)                   - Python Imaging Library\n",
      "django-media-pil (1.0.0)         - Simple widget for image manipulations by\n",
      "                                   pillow in the Django Admin\n",
      "large-image-source-pil (1.0.4)   - A Pillow tilesource for large_image\n",
      "pilhelp (1.1)                    - PIL helpers\n",
      "pils (0.1.25.post81)             - PILS - Python uTILS\n",
      "astc-codec (1.0.2)               - ASTC decoder for PIL\n",
      "astc-decomp (1.0.3)              - ASTC decoder for PIL\n",
      "pvrtc-decoder (1.0.2)            - A PVRTC decoder for PIL\n",
      "image_optimizer (0.3.6)          - PIL based image optimizer\n",
      "aggdraw (1.3.11)                 - High quality drawing interface for PIL.\n",
      "pilwmf (1.0b2-20040224)          - PIL WMF/EMF driver for Windows\n",
      "medialog.imageexport (0.5)       - Export Plone images pil scale imagescale\n",
      "Pyresize (0.2)                   - a cli application for resizing images,\n",
      "                                   based on PIL\n",
      "pydecorate (0.2.1)               - Decorating PIL images: logos, texts,\n",
      "                                   pallettes\n",
      "pillowfight (0.3)                - Eases the transition from PIL to Pillow for\n",
      "                                   projects.\n",
      "NativeImaging (0.0.10)           - PIL-like interface for system imaging\n",
      "                                   libraries\n",
      "generate_captcha (0.0.4)         - Generate verification code based on Pil\n",
      "                                   library.\n",
      "leia (0.1.0)                     - leia is a layering and compositing library\n",
      "                                   for Python and PIL images.\n",
      "pidi-display-tk (0.1.0)          - pidi plugin to display output using PIL and\n",
      "                                   Tk.\n",
      "projectile (0.0.6)               - A tile-on-demand tile server built with PIL\n",
      "                                   and Tornado\n",
      "elaphe (0.6.0)                   - Generates various barcodes using barcode.ps\n",
      "                                   and PIL/Pillow\n",
      "pillowcase (2.0.0)               - Smooths out installation issues due to the\n",
      "                                   PIL/Pillow fork.\n",
      "imageloader (0.0.5)              - An image loader library which provides a\n",
      "                                   subset of PIL interface.\n",
      "imgview (1.0)                    - A simple Tkinter+PIL image viewer with some\n",
      "                                   handy hotkeys.\n",
      "imdirect (0.5.0)                 - PIL extension performing automatic rotation\n",
      "                                   of opened JPEG images\n",
      "quickchart (0.0.1)               - QuickChart is a lightweight library for\n",
      "                                   drawing graphs and tables using just PIL.\n",
      "moving_pictures (0.1)            - Create a movie from a sequence of images in\n",
      "                                   Python (uses PIL and ffmpeg)\n",
      "img_rotate (0.1.1)               - Rotates PIL Image instances after EXIF-\n",
      "                                   tagged image orientation\n",
      "pilwrapper (0.1.0)               - A newbie-friendly wrapper around PIL/Pillow\n",
      "                                   for basic image access\n",
      "ImageToolsMadeEasy (2019.12.30)  - Tools to simplify using face detection and\n",
      "                                   ArUco recognition with PIL image objects\n",
      "pybmp (1.0.1)                    - BMP image handler for Python (to numpy\n",
      "                                   ndarray or PIL) native C .pyd\n",
      "numpy2gif (1.0)                  - Convert single and multiple numpy images to\n",
      "                                   a gif image without PIL or pillow\n",
      "reBarcode (0.1.0)                - Create standard barcodes with Python. No\n",
      "                                   external modules needed (optional PIL\n",
      "                                   support included).\n",
      "watermarker (1.1)                - Library for add text watermarks to images\n",
      "                                   using PIL, support sorl-thumbnail\n",
      "                                   integration\n",
      "tiqetsbarcode (0.8.0)            - Create standard barcodes with Python. No\n",
      "                                   external modules needed (optional PIL\n",
      "                                   support included).\n",
      "viivakoodi (0.8.0)               - Create standard barcodes with Python. No\n",
      "                                   external modules needed (optional PIL\n",
      "                                   support included).\n",
      "pyxpm (1.0.1)                    - XPM image file loader for Python (to numpy\n",
      "                                   ndarray or PIL) native C .pyd\n",
      "pyBarcode (0.8b1)                - Create standard barcodes with Python. No\n",
      "                                   external modules needed (optional PIL\n",
      "                                   support included).\n",
      "gif2numpy (1.3)                  - Convert single and multiple frame gif\n",
      "                                   images to numpy images or to OpenCV without\n",
      "                                   PIL or pillow\n",
      "pdf2image (1.12.1)               - A wrapper around the pdftoppm and\n",
      "                                   pdftocairo command line tools to convert\n",
      "                                   PDF to a PIL Image list.\n",
      "GifTiffLoader (0.2.4)            - automatically load multi-dimensional Tiff\n",
      "                                   and Gif files and file sequences as numpy\n",
      "                                   arrays using PIL\n",
      "PILasOPENCV (2.7)                - Wrapper for Image functions which are used\n",
      "                                   and called in the manner of the famous\n",
      "                                   PIL/Pillow module but work internally with\n",
      "                                   OpenCV.\n",
      "gpsphoto (2.2.3)                 - Returns, Modifies, or Removes GPS Data from\n",
      "                                   Exif Data in jpeg and tiff photos. Requires\n",
      "                                   ExifRead, piexif, and PIL.\n",
      "OleFileIO_PL (0.42.1)            - Python package to parse, read and write\n",
      "                                   Microsoft OLE2 files (Structured Storage or\n",
      "                                   Compound Document, Microsoft Office) -\n",
      "                                   Improved version of the OleFileIO module\n",
      "                                   from PIL, the Python Image Library.\n"
     ]
    }
   ],
   "source": [
    "!pip search pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "2.2.0-dev20200325\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import IPython.display as display\n",
    "#from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print(tf.test.gpu_device_name())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(5,5),activation='relu',\n",
    "                                 input_shape=(28, 28,1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo modelo com mais camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(5,5),activation='relu', input_shape=(28,28,1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalmente, redes CNNs são compostas por uma sequência de camadas de comvolução e pooling\n",
    "* Após esse sequência é normal uma camada densa com ativação softmax para classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo dados de entrada a partir de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat.10447.jpg  cat.10627.jpg  cat.10807.jpg  cat.10988.jpg  cat.11167.jpg\r\n",
      "cat.10448.jpg  cat.10628.jpg  cat.10808.jpg  cat.10989.jpg  cat.11168.jpg\r\n",
      "cat.10449.jpg  cat.10629.jpg  cat.10809.jpg  cat.1099.jpg   cat.11169.jpg\r\n",
      "cat.1045.jpg   cat.1063.jpg   cat.1081.jpg   cat.10990.jpg  cat.1117.jpg\r\n",
      "cat.10450.jpg  cat.10630.jpg  cat.10810.jpg  cat.10991.jpg  cat.11170.jpg\r\n",
      "cat.10451.jpg  cat.10631.jpg  cat.10811.jpg  cat.10992.jpg  cat.11171.jpg\r\n",
      "cat.10452.jpg  cat.10632.jpg  cat.10812.jpg  cat.10993.jpg  cat.11172.jpg\r\n",
      "cat.10453.jpg  cat.10633.jpg  cat.10813.jpg  cat.10994.jpg  cat.11173.jpg\r\n",
      "cat.10454.jpg  cat.10634.jpg  cat.10814.jpg  cat.10995.jpg  cat.11174.jpg\r\n",
      "cat.10455.jpg  cat.10635.jpg  cat.10815.jpg  cat.10996.jpg  cat.11175.jpg\r\n",
      "cat.10456.jpg  cat.10636.jpg  cat.10816.jpg  cat.10997.jpg  cat.11176.jpg\r\n",
      "cat.10457.jpg  cat.10637.jpg  cat.10817.jpg  cat.10998.jpg  cat.11177.jpg\r\n",
      "cat.10458.jpg  cat.10638.jpg  cat.10818.jpg  cat.10999.jpg  cat.11178.jpg\r\n",
      "cat.10459.jpg  cat.10639.jpg  cat.10819.jpg  cat.11.jpg     cat.11179.jpg\r\n",
      "cat.1046.jpg   cat.1064.jpg   cat.1082.jpg   cat.110.jpg    cat.1118.jpg\r\n",
      "cat.10460.jpg  cat.10640.jpg  cat.10820.jpg  cat.1100.jpg   cat.11180.jpg\r\n",
      "cat.10461.jpg  cat.10641.jpg  cat.10821.jpg  cat.11000.jpg  cat.11181.jpg\r\n",
      "cat.10462.jpg  cat.10642.jpg  cat.10822.jpg  cat.11001.jpg  cat.11182.jpg\r\n",
      "cat.10463.jpg  cat.10643.jpg  cat.10823.jpg  cat.11002.jpg  cat.11183.jpg\r\n",
      "cat.10464.jpg  cat.10644.jpg  cat.10824.jpg  cat.11003.jpg  cat.11184.jpg\r\n",
      "cat.10465.jpg  cat.10645.jpg  cat.10825.jpg  cat.11004.jpg  cat.11185.jpg\r\n",
      "cat.10466.jpg  cat.10646.jpg  cat.10826.jpg  cat.11005.jpg  cat.11186.jpg\r\n",
      "cat.10467.jpg  cat.10647.jpg  cat.10827.jpg  cat.11006.jpg  cat.11187.jpg\r\n",
      "cat.10468.jpg  cat.10648.jpg  cat.10828.jpg  cat.11007.jpg  cat.11188.jpg\r\n",
      "cat.10469.jpg  cat.10649.jpg  cat.10829.jpg  cat.11008.jpg  cat.11189.jpg\r\n",
      "cat.1047.jpg   cat.1065.jpg   cat.1083.jpg   cat.11009.jpg  cat.1119.jpg\r\n",
      "cat.10470.jpg  cat.10650.jpg  cat.10830.jpg  cat.1101.jpg   cat.11190.jpg\r\n",
      "cat.10471.jpg  cat.10651.jpg  cat.10831.jpg  cat.11010.jpg  cat.11191.jpg\r\n",
      "cat.10472.jpg  cat.10652.jpg  cat.10832.jpg  cat.11011.jpg  cat.11192.jpg\r\n",
      "cat.10473.jpg  cat.10653.jpg  cat.10833.jpg  cat.11012.jpg  cat.11193.jpg\r\n",
      "cat.10474.jpg  cat.10654.jpg  cat.10834.jpg  cat.11013.jpg  cat.11194.jpg\r\n",
      "cat.10475.jpg  cat.10655.jpg  cat.10835.jpg  cat.11014.jpg  cat.11195.jpg\r\n",
      "cat.10476.jpg  cat.10656.jpg  cat.10836.jpg  cat.11015.jpg  cat.11196.jpg\r\n",
      "cat.10477.jpg  cat.10657.jpg  cat.10837.jpg  cat.11016.jpg  cat.11197.jpg\r\n",
      "cat.10478.jpg  cat.10658.jpg  cat.10838.jpg  cat.11017.jpg  cat.11198.jpg\r\n",
      "cat.10479.jpg  cat.10659.jpg  cat.10839.jpg  cat.11018.jpg  cat.11199.jpg\r\n",
      "cat.1048.jpg   cat.1066.jpg   cat.1084.jpg   cat.11019.jpg  cat.112.jpg\r\n",
      "cat.10480.jpg  cat.10660.jpg  cat.10840.jpg  cat.1102.jpg   cat.1120.jpg\r\n",
      "cat.10481.jpg  cat.10661.jpg  cat.10841.jpg  cat.11020.jpg  cat.11200.jpg\r\n",
      "cat.10482.jpg  cat.10662.jpg  cat.10842.jpg  cat.11021.jpg  cat.11201.jpg\r\n",
      "cat.10483.jpg  cat.10663.jpg  cat.10843.jpg  cat.11022.jpg  cat.11202.jpg\r\n",
      "cat.10484.jpg  cat.10664.jpg  cat.10844.jpg  cat.11023.jpg  cat.11203.jpg\r\n",
      "cat.10485.jpg  cat.10665.jpg  cat.10845.jpg  cat.11024.jpg  cat.11204.jpg\r\n",
      "cat.10486.jpg  cat.10666.jpg  cat.10846.jpg  cat.11025.jpg  cat.11205.jpg\r\n",
      "cat.10487.jpg  cat.10667.jpg  cat.10847.jpg  cat.11026.jpg  cat.11206.jpg\r\n",
      "cat.10488.jpg  cat.10668.jpg  cat.10848.jpg  cat.11027.jpg  cat.11207.jpg\r\n",
      "cat.10489.jpg  cat.10669.jpg  cat.10849.jpg  cat.11028.jpg  cat.11208.jpg\r\n",
      "cat.1049.jpg   cat.1067.jpg   cat.1085.jpg   cat.11029.jpg  cat.11209.jpg\r\n",
      "cat.10490.jpg  cat.10670.jpg  cat.10850.jpg  cat.1103.jpg   cat.1121.jpg\r\n",
      "cat.10491.jpg  cat.10671.jpg  cat.10851.jpg  cat.11030.jpg  cat.11210.jpg\r\n",
      "cat.10492.jpg  cat.10672.jpg  cat.10852.jpg  cat.11031.jpg  cat.11211.jpg\r\n",
      "cat.10493.jpg  cat.10673.jpg  cat.10853.jpg  cat.11032.jpg  cat.11212.jpg\r\n",
      "cat.10494.jpg  cat.10674.jpg  cat.10854.jpg  cat.11033.jpg  cat.11213.jpg\r\n",
      "cat.10495.jpg  cat.10675.jpg  cat.10855.jpg  cat.11034.jpg  cat.11214.jpg\r\n",
      "cat.10496.jpg  cat.10676.jpg  cat.10856.jpg  cat.11035.jpg  cat.11215.jpg\r\n",
      "cat.10497.jpg  cat.10677.jpg  cat.10857.jpg  cat.11036.jpg  cat.11216.jpg\r\n",
      "cat.10498.jpg  cat.10678.jpg  cat.10858.jpg  cat.11037.jpg  cat.11217.jpg\r\n",
      "cat.10499.jpg  cat.10679.jpg  cat.10859.jpg  cat.11038.jpg  cat.11218.jpg\r\n",
      "cat.105.jpg    cat.1068.jpg   cat.1086.jpg   cat.11039.jpg  cat.11219.jpg\r\n",
      "cat.1050.jpg   cat.10680.jpg  cat.10860.jpg  cat.1104.jpg   cat.1122.jpg\r\n",
      "cat.10500.jpg  cat.10681.jpg  cat.10861.jpg  cat.11040.jpg  cat.11220.jpg\r\n",
      "cat.10501.jpg  cat.10682.jpg  cat.10862.jpg  cat.11041.jpg  cat.11221.jpg\r\n",
      "cat.10502.jpg  cat.10683.jpg  cat.10863.jpg  cat.11042.jpg  cat.11222.jpg\r\n",
      "cat.10503.jpg  cat.10684.jpg  cat.10864.jpg  cat.11043.jpg  cat.11223.jpg\r\n",
      "cat.10504.jpg  cat.10685.jpg  cat.10865.jpg  cat.11044.jpg  cat.11224.jpg\r\n",
      "cat.10505.jpg  cat.10686.jpg  cat.10866.jpg  cat.11045.jpg  cat.11225.jpg\r\n",
      "cat.10506.jpg  cat.10687.jpg  cat.10867.jpg  cat.11046.jpg  cat.11226.jpg\r\n",
      "cat.10507.jpg  cat.10688.jpg  cat.10868.jpg  cat.11047.jpg  cat.11227.jpg\r\n",
      "cat.10508.jpg  cat.10689.jpg  cat.10869.jpg  cat.11048.jpg  cat.11228.jpg\r\n",
      "cat.10509.jpg  cat.1069.jpg   cat.1087.jpg   cat.11049.jpg  cat.11229.jpg\r\n",
      "cat.1051.jpg   cat.10690.jpg  cat.10870.jpg  cat.1105.jpg   cat.1123.jpg\r\n",
      "cat.10510.jpg  cat.10691.jpg  cat.10871.jpg  cat.11050.jpg  cat.11230.jpg\r\n",
      "cat.10511.jpg  cat.10692.jpg  cat.10872.jpg  cat.11051.jpg  cat.11231.jpg\r\n",
      "cat.10512.jpg  cat.10693.jpg  cat.10873.jpg  cat.11052.jpg  cat.11232.jpg\r\n",
      "cat.10513.jpg  cat.10694.jpg  cat.10874.jpg  cat.11053.jpg  cat.11233.jpg\r\n",
      "cat.10514.jpg  cat.10695.jpg  cat.10875.jpg  cat.11054.jpg  cat.11234.jpg\r\n",
      "cat.10515.jpg  cat.10696.jpg  cat.10876.jpg  cat.11055.jpg  cat.11235.jpg\r\n",
      "cat.10516.jpg  cat.10697.jpg  cat.10877.jpg  cat.11056.jpg  cat.11236.jpg\r\n",
      "cat.10517.jpg  cat.10698.jpg  cat.10878.jpg  cat.11057.jpg  cat.11237.jpg\r\n",
      "cat.10518.jpg  cat.10699.jpg  cat.10879.jpg  cat.11058.jpg  cat.11238.jpg\r\n",
      "cat.10519.jpg  cat.107.jpg    cat.1088.jpg   cat.11059.jpg  cat.11239.jpg\r\n",
      "cat.1052.jpg   cat.1070.jpg   cat.10880.jpg  cat.1106.jpg   cat.1124.jpg\r\n",
      "cat.10520.jpg  cat.10700.jpg  cat.10881.jpg  cat.11060.jpg  cat.11240.jpg\r\n",
      "cat.10521.jpg  cat.10701.jpg  cat.10882.jpg  cat.11061.jpg  cat.11241.jpg\r\n",
      "cat.10522.jpg  cat.10702.jpg  cat.10883.jpg  cat.11062.jpg  cat.11242.jpg\r\n",
      "cat.10523.jpg  cat.10703.jpg  cat.10884.jpg  cat.11063.jpg  cat.11243.jpg\r\n",
      "cat.10524.jpg  cat.10704.jpg  cat.10885.jpg  cat.11064.jpg  cat.11244.jpg\r\n",
      "cat.10525.jpg  cat.10705.jpg  cat.10886.jpg  cat.11065.jpg  cat.11245.jpg\r\n",
      "cat.10526.jpg  cat.10706.jpg  cat.10887.jpg  cat.11066.jpg  cat.11246.jpg\r\n",
      "cat.10527.jpg  cat.10707.jpg  cat.10888.jpg  cat.11067.jpg  cat.11247.jpg\r\n",
      "cat.10528.jpg  cat.10708.jpg  cat.10889.jpg  cat.11068.jpg  cat.11248.jpg\r\n",
      "cat.10529.jpg  cat.10709.jpg  cat.1089.jpg   cat.11069.jpg  cat.11249.jpg\r\n",
      "cat.1053.jpg   cat.1071.jpg   cat.10890.jpg  cat.1107.jpg   cat.1125.jpg\r\n",
      "cat.10530.jpg  cat.10710.jpg  cat.10891.jpg  cat.11070.jpg  cat.11250.jpg\r\n",
      "cat.10531.jpg  cat.10711.jpg  cat.10892.jpg  cat.11071.jpg  cat.11251.jpg\r\n",
      "cat.10532.jpg  cat.10712.jpg  cat.10893.jpg  cat.11072.jpg  cat.11252.jpg\r\n",
      "cat.10533.jpg  cat.10713.jpg  cat.10894.jpg  cat.11073.jpg  cat.11253.jpg\r\n",
      "cat.10534.jpg  cat.10714.jpg  cat.10895.jpg  cat.11074.jpg  cat.11254.jpg\r\n",
      "cat.10535.jpg  cat.10715.jpg  cat.10896.jpg  cat.11075.jpg  cat.11255.jpg\r\n",
      "cat.10536.jpg  cat.10716.jpg  cat.10897.jpg  cat.11076.jpg  cat.11256.jpg\r\n",
      "cat.10537.jpg  cat.10717.jpg  cat.10898.jpg  cat.11077.jpg  cat.11257.jpg\r\n",
      "cat.10538.jpg  cat.10718.jpg  cat.10899.jpg  cat.11078.jpg  cat.11258.jpg\r\n",
      "cat.10539.jpg  cat.10719.jpg  cat.109.jpg    cat.11079.jpg  cat.11259.jpg\r\n",
      "cat.1054.jpg   cat.1072.jpg   cat.1090.jpg   cat.1108.jpg   cat.1126.jpg\r\n",
      "cat.10540.jpg  cat.10720.jpg  cat.10900.jpg  cat.11080.jpg  cat.11260.jpg\r\n",
      "cat.10541.jpg  cat.10721.jpg  cat.10901.jpg  cat.11081.jpg  cat.11261.jpg\r\n",
      "cat.10542.jpg  cat.10722.jpg  cat.10902.jpg  cat.11082.jpg  cat.11262.jpg\r\n",
      "cat.10543.jpg  cat.10723.jpg  cat.10903.jpg  cat.11083.jpg  cat.11263.jpg\r\n",
      "cat.10544.jpg  cat.10724.jpg  cat.10904.jpg  cat.11084.jpg  cat.11264.jpg\r\n",
      "cat.10545.jpg  cat.10725.jpg  cat.10905.jpg  cat.11085.jpg  cat.11265.jpg\r\n",
      "cat.10546.jpg  cat.10726.jpg  cat.10906.jpg  cat.11086.jpg  cat.11266.jpg\r\n",
      "cat.10547.jpg  cat.10727.jpg  cat.10907.jpg  cat.11087.jpg  cat.11267.jpg\r\n",
      "cat.10548.jpg  cat.10728.jpg  cat.10908.jpg  cat.11088.jpg  cat.11268.jpg\r\n",
      "cat.10549.jpg  cat.10729.jpg  cat.10909.jpg  cat.11089.jpg  cat.11269.jpg\r\n",
      "cat.1055.jpg   cat.1073.jpg   cat.1091.jpg   cat.1109.jpg   cat.1127.jpg\r\n",
      "cat.10550.jpg  cat.10730.jpg  cat.10910.jpg  cat.11090.jpg  cat.11270.jpg\r\n",
      "cat.10551.jpg  cat.10731.jpg  cat.10911.jpg  cat.11091.jpg  cat.11271.jpg\r\n",
      "cat.10552.jpg  cat.10732.jpg  cat.10912.jpg  cat.11092.jpg  cat.11272.jpg\r\n",
      "cat.10553.jpg  cat.10733.jpg  cat.10913.jpg  cat.11093.jpg  cat.11273.jpg\r\n",
      "cat.10554.jpg  cat.10734.jpg  cat.10914.jpg  cat.11094.jpg  cat.11274.jpg\r\n",
      "cat.10555.jpg  cat.10735.jpg  cat.10915.jpg  cat.11095.jpg  cat.11275.jpg\r\n",
      "cat.10556.jpg  cat.10736.jpg  cat.10916.jpg  cat.11096.jpg  cat.11276.jpg\r\n",
      "cat.10557.jpg  cat.10737.jpg  cat.10917.jpg  cat.11097.jpg  cat.11277.jpg\r\n",
      "cat.10558.jpg  cat.10738.jpg  cat.10918.jpg  cat.11098.jpg  cat.11278.jpg\r\n",
      "cat.10559.jpg  cat.10739.jpg  cat.10919.jpg  cat.11099.jpg  cat.11279.jpg\r\n",
      "cat.1056.jpg   cat.1074.jpg   cat.1092.jpg   cat.111.jpg    cat.1128.jpg\r\n",
      "cat.10560.jpg  cat.10740.jpg  cat.10920.jpg  cat.1110.jpg   cat.11280.jpg\r\n",
      "cat.10561.jpg  cat.10741.jpg  cat.10921.jpg  cat.11100.jpg  cat.11281.jpg\r\n",
      "cat.10562.jpg  cat.10742.jpg  cat.10922.jpg  cat.11101.jpg  cat.11282.jpg\r\n",
      "cat.10563.jpg  cat.10743.jpg  cat.10923.jpg  cat.11102.jpg  cat.11283.jpg\r\n",
      "cat.10564.jpg  cat.10744.jpg  cat.10924.jpg  cat.11103.jpg  cat.11284.jpg\r\n",
      "cat.10565.jpg  cat.10745.jpg  cat.10925.jpg  cat.11104.jpg  cat.11285.jpg\r\n",
      "cat.10566.jpg  cat.10746.jpg  cat.10926.jpg  cat.11105.jpg  cat.11286.jpg\r\n",
      "cat.10567.jpg  cat.10747.jpg  cat.10927.jpg  cat.11106.jpg  cat.11287.jpg\r\n",
      "cat.10568.jpg  cat.10748.jpg  cat.10928.jpg  cat.11107.jpg  cat.11288.jpg\r\n",
      "cat.10569.jpg  cat.10749.jpg  cat.10929.jpg  cat.11108.jpg  cat.11289.jpg\r\n",
      "cat.1057.jpg   cat.1075.jpg   cat.1093.jpg   cat.11109.jpg  cat.1129.jpg\r\n",
      "cat.10570.jpg  cat.10750.jpg  cat.10930.jpg  cat.1111.jpg   cat.11290.jpg\r\n",
      "cat.10571.jpg  cat.10751.jpg  cat.10931.jpg  cat.11110.jpg  cat.11291.jpg\r\n",
      "cat.10572.jpg  cat.10752.jpg  cat.10932.jpg  cat.11111.jpg  cat.11292.jpg\r\n",
      "cat.10573.jpg  cat.10753.jpg  cat.10933.jpg  cat.11112.jpg  cat.11293.jpg\r\n",
      "cat.10574.jpg  cat.10754.jpg  cat.10934.jpg  cat.11113.jpg  cat.11294.jpg\r\n",
      "cat.10575.jpg  cat.10755.jpg  cat.10935.jpg  cat.11114.jpg  cat.11295.jpg\r\n",
      "cat.10576.jpg  cat.10756.jpg  cat.10936.jpg  cat.11115.jpg  cat.11296.jpg\r\n",
      "cat.10577.jpg  cat.10757.jpg  cat.10937.jpg  cat.11116.jpg  cat.11297.jpg\r\n",
      "cat.10578.jpg  cat.10758.jpg  cat.10938.jpg  cat.11117.jpg  cat.11298.jpg\r\n",
      "cat.10579.jpg  cat.10759.jpg  cat.10939.jpg  cat.11118.jpg  cat.11299.jpg\r\n",
      "cat.1058.jpg   cat.1076.jpg   cat.1094.jpg   cat.11119.jpg  cat.113.jpg\r\n",
      "cat.10580.jpg  cat.10760.jpg  cat.10940.jpg  cat.1112.jpg   cat.1130.jpg\r\n",
      "cat.10581.jpg  cat.10761.jpg  cat.10941.jpg  cat.11120.jpg  cat.11300.jpg\r\n",
      "cat.10582.jpg  cat.10762.jpg  cat.10942.jpg  cat.11121.jpg  cat.11301.jpg\r\n",
      "cat.10583.jpg  cat.10763.jpg  cat.10943.jpg  cat.11122.jpg  cat.11302.jpg\r\n",
      "cat.10584.jpg  cat.10764.jpg  cat.10944.jpg  cat.11123.jpg  cat.11303.jpg\r\n",
      "cat.10585.jpg  cat.10765.jpg  cat.10945.jpg  cat.11124.jpg  cat.11304.jpg\r\n",
      "cat.10586.jpg  cat.10766.jpg  cat.10946.jpg  cat.11125.jpg  cat.11305.jpg\r\n",
      "cat.10587.jpg  cat.10767.jpg  cat.10947.jpg  cat.11126.jpg  cat.11306.jpg\r\n",
      "cat.10588.jpg  cat.10768.jpg  cat.10948.jpg  cat.11127.jpg  cat.11307.jpg\r\n",
      "cat.10589.jpg  cat.10769.jpg  cat.10949.jpg  cat.11128.jpg  cat.11308.jpg\r\n",
      "cat.1059.jpg   cat.1077.jpg   cat.1095.jpg   cat.11129.jpg  cat.11309.jpg\r\n",
      "cat.10590.jpg  cat.10770.jpg  cat.10950.jpg  cat.1113.jpg   cat.1131.jpg\r\n",
      "cat.10591.jpg  cat.10771.jpg  cat.10951.jpg  cat.11130.jpg  cat.11310.jpg\r\n",
      "cat.10592.jpg  cat.10772.jpg  cat.10952.jpg  cat.11131.jpg  cat.11311.jpg\r\n",
      "cat.10593.jpg  cat.10773.jpg  cat.10953.jpg  cat.11132.jpg  cat.11312.jpg\r\n",
      "cat.10594.jpg  cat.10774.jpg  cat.10954.jpg  cat.11133.jpg  cat.11313.jpg\r\n",
      "cat.10595.jpg  cat.10775.jpg  cat.10955.jpg  cat.11134.jpg  cat.11314.jpg\r\n",
      "cat.10596.jpg  cat.10776.jpg  cat.10956.jpg  cat.11135.jpg  cat.11315.jpg\r\n",
      "cat.10597.jpg  cat.10777.jpg  cat.10957.jpg  cat.11136.jpg  cat.11316.jpg\r\n",
      "cat.10598.jpg  cat.10778.jpg  cat.10958.jpg  cat.11137.jpg  cat.11317.jpg\r\n",
      "cat.10599.jpg  cat.10779.jpg  cat.10959.jpg  cat.11138.jpg  cat.11318.jpg\r\n",
      "cat.106.jpg    cat.1078.jpg   cat.1096.jpg   cat.11139.jpg  cat.11319.jpg\r\n",
      "cat.1060.jpg   cat.10780.jpg  cat.10960.jpg  cat.1114.jpg   cat.1132.jpg\r\n",
      "cat.10600.jpg  cat.10781.jpg  cat.10961.jpg  cat.11140.jpg  cat.11320.jpg\r\n",
      "cat.10601.jpg  cat.10782.jpg  cat.10962.jpg  cat.11141.jpg  cat.11321.jpg\r\n",
      "cat.10602.jpg  cat.10783.jpg  cat.10963.jpg  cat.11142.jpg  cat.11322.jpg\r\n",
      "cat.10603.jpg  cat.10784.jpg  cat.10964.jpg  cat.11143.jpg  cat.11323.jpg\r\n",
      "cat.10604.jpg  cat.10785.jpg  cat.10965.jpg  cat.11144.jpg  cat.11324.jpg\r\n",
      "cat.10605.jpg  cat.10786.jpg  cat.10966.jpg  cat.11145.jpg  cat.11325.jpg\r\n",
      "cat.10606.jpg  cat.10787.jpg  cat.10967.jpg  cat.11146.jpg  cat.11326.jpg\r\n",
      "cat.10607.jpg  cat.10788.jpg  cat.10968.jpg  cat.11147.jpg  cat.11327.jpg\r\n",
      "cat.10608.jpg  cat.10789.jpg  cat.10969.jpg  cat.11148.jpg  cat.11328.jpg\r\n",
      "cat.10609.jpg  cat.1079.jpg   cat.1097.jpg   cat.11149.jpg  cat.11329.jpg\r\n",
      "cat.1061.jpg   cat.10790.jpg  cat.10970.jpg  cat.1115.jpg   cat.1133.jpg\r\n",
      "cat.10610.jpg  cat.10791.jpg  cat.10971.jpg  cat.11150.jpg  cat.11330.jpg\r\n",
      "cat.10611.jpg  cat.10792.jpg  cat.10972.jpg  cat.11151.jpg  cat.11331.jpg\r\n",
      "cat.10612.jpg  cat.10793.jpg  cat.10973.jpg  cat.11152.jpg  cat.11332.jpg\r\n",
      "cat.10613.jpg  cat.10794.jpg  cat.10974.jpg  cat.11153.jpg  cat.11333.jpg\r\n",
      "cat.10614.jpg  cat.10795.jpg  cat.10975.jpg  cat.11154.jpg  cat.11334.jpg\r\n",
      "cat.10615.jpg  cat.10796.jpg  cat.10976.jpg  cat.11155.jpg  cat.11335.jpg\r\n",
      "cat.10616.jpg  cat.10797.jpg  cat.10977.jpg  cat.11156.jpg  cat.11336.jpg\r\n",
      "cat.10617.jpg  cat.10798.jpg  cat.10978.jpg  cat.11157.jpg  cat.11337.jpg\r\n",
      "cat.10618.jpg  cat.10799.jpg  cat.10979.jpg  cat.11158.jpg  cat.11338.jpg\r\n",
      "cat.10619.jpg  cat.108.jpg    cat.1098.jpg   cat.11159.jpg  cat.11339.jpg\r\n",
      "cat.1062.jpg   cat.1080.jpg   cat.10980.jpg  cat.1116.jpg   cat.1134.jpg\r\n",
      "cat.10620.jpg  cat.10800.jpg  cat.10981.jpg  cat.11160.jpg  cat.11340.jpg\r\n",
      "cat.10621.jpg  cat.10801.jpg  cat.10982.jpg  cat.11161.jpg  cat.11341.jpg\r\n",
      "cat.10622.jpg  cat.10802.jpg  cat.10983.jpg  cat.11162.jpg  cat.11342.jpg\r\n",
      "cat.10623.jpg  cat.10803.jpg  cat.10984.jpg  cat.11163.jpg  cat.11343.jpg\r\n",
      "cat.10624.jpg  cat.10804.jpg  cat.10985.jpg  cat.11164.jpg  cat.11344.jpg\r\n",
      "cat.10625.jpg  cat.10805.jpg  cat.10986.jpg  cat.11165.jpg  cat.11345.jpg\r\n",
      "cat.10626.jpg  cat.10806.jpg  cat.10987.jpg  cat.11166.jpg  cat.11346.jpg\r\n"
     ]
    }
   ],
   "source": [
    "#!ls ../dogsCatsDB/cats/train\n",
    "!ls ../dogsCatsDB/cats/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../dogsCatsDB/train\n",
      "total training cat images: 21000\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#train_dir='../dogsCatsDB/cats/train'\n",
    "#validation_dir='../dogsCatsDB/cats/validation'\n",
    "\n",
    "base_dir = '../dogsCatsDB/'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "print(train_dir)\n",
    "print('total training cat images:', len(os.listdir(train_dir)))\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir, # This is the target directory\n",
    "        target_size=(150, 150), # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        class_mode='binary') # Since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "#validation_generator = test_datagen.flow_from_directory(\n",
    "#        validation_dir,\n",
    "#        target_size=(150, 150),\n",
    "#        batch_size=20,\n",
    "#        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 - Usando convnets com pequenos conjuntos de dados\n",
    "\n",
    "Treinar redes convolucionais com poucos dados é comum, desde que seja uma amostra significativa\n",
    "\n",
    "Treino do modelo com uma convnet simples Treino usando data augmentation (para resolver problema de overfit)\n",
    "\n",
    "Na próxima seção\n",
    "\n",
    "feature extraction com uma rede pré-treinada\n",
    "fine-tuning com uma rede pré-treinada\n",
    "Dataset para Deep Learning\n",
    "\n",
    "Normalmente são executados com base em grandes conjuntos de dados\n",
    "Para processamento de imagens, em muitos casos, alguns exemplos podem ser suficientes para generalizar um problema\n",
    "Porque convnets aprendem features locais que são translation-invariant, portanto são eficientes em generalizar um problema\n",
    "Outro aspecto das redes neurais, é que devido a capacidade de generalização, em alguns casos, é comum que uma rede seja reutilizada para que seja feito aprofundamento em outros domínios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de imagens no diretório 3670\n",
      "Quantidade de classes ['daisy' 'tulips' 'dandelion' 'sunflowers' 'roses']\n",
      "Found 3670 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = tf.keras.utils.get_file(origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "                                         fname='flower_photos', untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(\"Quantidade de imagens no diretório\",image_count)\n",
    "\n",
    "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
    "print(\"Quantidade de classes\",  CLASS_NAMES)\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n",
    "\n",
    "train_data_gen = image_generator.flow_from_directory(directory=str(data_dir),\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = list(CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.image.directory_iterator.DirectoryIterator'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n in range(25):\n",
    "      ax = plt.subplot(5,5,n+1)\n",
    "      plt.imshow(image_batch[n])\n",
    "      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
    "      plt.axis('off')\n",
    "      #print(image_batch[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b51e3c6b3edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(image_batch.shape )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(label_batch.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshow_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                            interpolation=self.interpolation)\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mcolor_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpil_image\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         raise ImportError('Could not import PIL.Image. '\n\u001b[0m\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    110\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(train_data_gen)\n",
    "#print(image_batch.shape )\n",
    "#print(label_batch.shape)\n",
    "show_batch(image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/tmp/.keras/datasets/flower_photos/tulips/486896118_bcc7b8e1d6.jpg'\n",
      "b'/tmp/.keras/datasets/flower_photos/daisy/14921511479_7b0a647795.jpg'\n",
      "b'/tmp/.keras/datasets/flower_photos/roses/5717319579_190e85c7d1_m.jpg'\n",
      "b'/tmp/.keras/datasets/flower_photos/roses/2265579414_2e00a8f265_n.jpg'\n",
      "b'/tmp/.keras/datasets/flower_photos/daisy/10437754174_22ec990b77_m.jpg'\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
    "for f in list_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == CLASS_NAMES\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "print(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.ParallelMapDataset"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labeled_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparando** dataset em arquivos para treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "  # This is a small dataset, only load it once, and keep it in memory.\n",
    "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  if cache:\n",
    "    if isinstance(cache, str):\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      ds = ds.cache()\n",
    "\n",
    "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "  # Repeat forever\n",
    "  ds = ds.repeat()\n",
    "\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filecache_ds = prepare_for_training(labeled_ds, cache=\"./flowers.tfcache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = tf.keras.utils.get_file(origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "                                         fname='flower_photos', untar=True)\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b2210381090f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m       \u001b[0mtrain_data_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#train_generator,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       epochs=30) #,\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0;31m#validation_data=validation_generator,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0;31m#validation_steps=50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m     \u001b[0massert_not_namedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    843\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                            interpolation=self.interpolation)\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mcolor_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpil_image\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         raise ImportError('Could not import PIL.Image. '\n\u001b[0m\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    110\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_data_gen, #train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30) #,\n",
    "      #validation_data=validation_generator,\n",
    "      #validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
    "CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roses = list(data_dir.glob('roses/*'))\n",
    "\n",
    "for image_path in roses[:3]:\n",
    "    display.display(Image.open(str(image_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 1./255 is to convert from uint8 to float32 in range [0,1].\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = image_generator.flow_from_directory(directory=str(data_dir),\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                     classes = list(CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n in range(25):\n",
    "      ax = plt.subplot(5,5,n+1)\n",
    "      plt.imshow(image_batch[n])\n",
    "      plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n",
    "      plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(train_data_gen)\n",
    "show_batch(image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
