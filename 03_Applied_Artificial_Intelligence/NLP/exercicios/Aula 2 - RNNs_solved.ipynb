{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já sabemos fazer o processamento inicial no texto e construir um classificador, vamos explorar as arquiteturas que permitam processar a sequência de texto mais efetivamente.\n",
    "[GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n",
    "[LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "\n",
    "Como estes algoritmos são capazes de processar sequências de caracteres, podemos considerar tarefas mais desafiadoras.\n",
    "\n",
    "Considere o problema de **Gerar Texto**.\n",
    "\n",
    "Na aula passada consideramos a tarefa de classificar o \"sentimento\" da sentenca. Agora vamos resolver um problema mais desafiador. \n",
    "\n",
    "Nesta aula vamos modelar o estilo de \"William Shakespeare\", treinando um modelo para que complete poemas \"como Shakespeare os escreveria\".\n",
    "\n",
    "Basicamente, o problema consiste em prever a próxima palavra dado o restante da frase, por exemplo:\n",
    "\n",
    "**x** *= \"That thereby beauty's rose might never\"*, **y** *= \"die\"*\n",
    "\n",
    "\n",
    "**x** *= \"Or who is he so fond will be the\"*, **y** *= \"tomb\"*\n",
    "\n",
    "Para tanto, preparamos um dataset com exemplos de sonetos escritos por Shakespeare:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from fairest creatures we desire increase,', \"that thereby beauty's rose might never die,\", 'but as the riper should by time decease,', 'his tender heir might bear his memory:', 'but thou, contracted to thine own bright eyes,']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data = open('./data/text_generation/sonnets.txt').read()\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "print(corpus[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como de costume, precisamos modelar esse problema de maneira que ele seja tratável com uma rede neural.\n",
    "\n",
    "Agora já sabemos utilizar a classe [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) para codificar strings, porém no problema anterior tínhamos que fazer uma classificacão binária de um texto de tamanho máximo constante.\n",
    "\n",
    "Agora, o modelo deve responder com uma palavra, como podemos modificar nossa rede neural para resolver este problema?\n",
    "\n",
    "Antes de modificar a rede neural, vamos pensar como transformaremos o texto bruto no dataset para treinamento do modelo.\n",
    "\n",
    "Uma forma é se aproveitar da funcão [pad_sequence](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) com *padding='pre'* para que normalizemos o tamanho da string de forma que a \"última palavra\" (isto é, a palavra a ser predita pelo modelo) fique na última posicão e seja fácil de se extrair.\n",
    "\n",
    "por exemplo:\n",
    "\n",
    "\n",
    "`\n",
    "\"from fairest creatures we desire increase\" -> (Tokenizer) -> \n",
    "[1, 2, 3, 4, 5, 6] -> (padding) -> [0, 0, 1, 2, 3, 4, 5, 6]`\n",
    "\n",
    "Assim, *y* é facilmente acessível no índice `sentenca[-1]`.\n",
    "\n",
    "Portanto, o primeiro passo é utilizar o tokenizer para que o vocabulário seja definido:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[34, 417, 877, 166, 213, 517],\n",
       " [8, 878, 134, 351, 102, 156, 199],\n",
       " [16, 22, 2, 879, 61, 30, 48, 634],\n",
       " [25, 311, 635, 102, 200, 25, 278],\n",
       " [16, 10, 880, 3, 62, 85, 214, 53]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_vocab_size = 1000\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, lower=True)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "corpus_seqs = tokenizer.texts_to_sequences(corpus)\n",
    "corpus_seqs[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é organizar nossa base de treinamento. \n",
    "\n",
    "Como queremos que o modelo seja capaz de prever a próxima palavra em qualquer ponto da frase, vamos considerar todas as subfrases possíveis de se formar com cada linha de soneto.\n",
    "\n",
    "Por exemplo, para a frase `\"from fairest creatures we desire increase\"` as seguintes subfrases serão geradas\n",
    "\n",
    "`\n",
    "\"from fairest\"\n",
    "\"from fairest creatures\"\n",
    "\"from fairest creatures we\"\n",
    "\"from fairest creatures we desire\"\n",
    "\"from fairest creatures we desire increase\"\n",
    "`\n",
    "\n",
    "Ou, em sua versão codificada:\n",
    "\n",
    "`\n",
    "[35, 418]\n",
    "[35, 418, 878]\n",
    "[35, 418, 878, 167]\n",
    "[35, 418, 878, 167, 214]\n",
    "[35, 418, 878, 167, 214, 518]\n",
    "`\n",
    "\n",
    "Codifique uma funcao `process_corpus(seqs)` que vai transformar o corpus_seqs definido na célula acima criando as subfrases como descrito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34, 417], [34, 417, 877], [34, 417, 877, 166], [34, 417, 877, 166, 213], [34, 417, 877, 166, 213, 517], [8, 878], [8, 878, 134], [8, 878, 134, 351], [8, 878, 134, 351, 102], [8, 878, 134, 351, 102, 156]]\n"
     ]
    }
   ],
   "source": [
    "def process_corpus(seqs):\n",
    "    res_seqs = []\n",
    "    for seq in seqs:\n",
    "        if len(seq) > 1:\n",
    "            for i in range(1, len(seq)):\n",
    "                res_seqs.append(seq[0:i+1])\n",
    "    return res_seqs\n",
    "\n",
    "proc_corpus = process_corpus(corpus_seqs)\n",
    "print(proc_corpus[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos fazer o processamento final das sentencas com [padding](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences).\n",
    "\n",
    "\n",
    "O faremos de modo que a \"palavra a ser predita\" fique no último índice, facilitando o processamento posterior quando precisarmos separar a base em \"x\" e \"y\".\n",
    "\n",
    "Note que aqui também devemos determinar o \"tamanho máximo da frase\", que efetivamente vai determinar até quantas palavras no passado o modelo vai olhar para definir a próxima palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0  34 417]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0  34 417 877]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0  34 417 877 166]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  34 417 877 166 213]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  34 417 877 166 213 517]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 30\n",
    "\n",
    "\n",
    "padded_dataset = pad_sequences(proc_corpus, maxlen=max_len + 1, padding='pre', truncating='pre')\n",
    "print(padded_dataset[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos os dados organizados de uma maneira mais palatável para uma rede neural.\n",
    "\n",
    "Diferentemente da rede construída para a tarefa de classificacão de sentimentos, aqui nossa rede neural precisa ter `max_vocab_size` saídas, que efetivamente significa que a saída da rede vai ser uma probabilidade de a próxima palavra ser cada uma das possiblidades em nosso vocábulo.\n",
    "\n",
    "Para o treinamento, é mais indicado que codifiquemos a palavra a ser prevista com um one-hot encoding, assim a saída da rede vai ser uma probabilidade para cada palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0  34]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0  34 417]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np\n",
    "\n",
    "X = np.array(padded_dataset[:,:-1])\n",
    "y = padded_dataset[:,-1]\n",
    "y = ku.to_categorical(y, num_classes = max_vocab_size)\n",
    "\n",
    "\n",
    "print(X[0:2])\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora sim, o processamento do texto está terminado e podemos treinar nossa rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 100)           100000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 30, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               25500     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              501000    \n",
      "=================================================================\n",
      "Total params: 737,100\n",
      "Trainable params: 737,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Dropout, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(max_vocab_size/2, activation='relu'))\n",
    "model.add(Dense(max_vocab_size, activation='softmax'))\n",
    "# Pick an optimizer\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12880 samples\n",
      "Epoch 1/100\n",
      "12880/12880 [==============================] - 26s 2ms/sample - loss: 5.9830 - accuracy: 0.0292\n",
      "Epoch 2/100\n",
      "12880/12880 [==============================] - 3s 246us/sample - loss: 5.7737 - accuracy: 0.0294\n",
      "Epoch 3/100\n",
      "12880/12880 [==============================] - 3s 231us/sample - loss: 5.6763 - accuracy: 0.0342\n",
      "Epoch 4/100\n",
      "12880/12880 [==============================] - 3s 256us/sample - loss: 5.6127 - accuracy: 0.0369\n",
      "Epoch 5/100\n",
      "12880/12880 [==============================] - 4s 275us/sample - loss: 5.5706 - accuracy: 0.0390\n",
      "Epoch 6/100\n",
      "12880/12880 [==============================] - 3s 269us/sample - loss: 5.5224 - accuracy: 0.0409\n",
      "Epoch 7/100\n",
      "12880/12880 [==============================] - 4s 301us/sample - loss: 5.4691 - accuracy: 0.0418\n",
      "Epoch 8/100\n",
      "12880/12880 [==============================] - 4s 275us/sample - loss: 5.4154 - accuracy: 0.0428\n",
      "Epoch 9/100\n",
      "12880/12880 [==============================] - 3s 251us/sample - loss: 5.3610 - accuracy: 0.0446\n",
      "Epoch 10/100\n",
      "12880/12880 [==============================] - 3s 264us/sample - loss: 5.2942 - accuracy: 0.0497\n",
      "Epoch 11/100\n",
      "12880/12880 [==============================] - 3s 246us/sample - loss: 5.2189 - accuracy: 0.0559\n",
      "Epoch 12/100\n",
      "12880/12880 [==============================] - 3s 265us/sample - loss: 5.1347 - accuracy: 0.0607\n",
      "Epoch 13/100\n",
      "12880/12880 [==============================] - 3s 261us/sample - loss: 5.0447 - accuracy: 0.0642\n",
      "Epoch 14/100\n",
      "12880/12880 [==============================] - 3s 250us/sample - loss: 4.9576 - accuracy: 0.0702\n",
      "Epoch 15/100\n",
      "12880/12880 [==============================] - 3s 261us/sample - loss: 4.8650 - accuracy: 0.0738\n",
      "Epoch 16/100\n",
      "12880/12880 [==============================] - 3s 245us/sample - loss: 4.7737 - accuracy: 0.0783\n",
      "Epoch 17/100\n",
      "12880/12880 [==============================] - 3s 246us/sample - loss: 4.6760 - accuracy: 0.0819\n",
      "Epoch 18/100\n",
      "12880/12880 [==============================] - 3s 250us/sample - loss: 4.5829 - accuracy: 0.0867\n",
      "Epoch 19/100\n",
      "12880/12880 [==============================] - 3s 247us/sample - loss: 4.4895 - accuracy: 0.0917\n",
      "Epoch 20/100\n",
      "12880/12880 [==============================] - 3s 254us/sample - loss: 4.3905 - accuracy: 0.0981\n",
      "Epoch 21/100\n",
      "12880/12880 [==============================] - 4s 276us/sample - loss: 4.2972 - accuracy: 0.1032\n",
      "Epoch 22/100\n",
      "12880/12880 [==============================] - 4s 279us/sample - loss: 4.2043 - accuracy: 0.1075\n",
      "Epoch 23/100\n",
      "12880/12880 [==============================] - 3s 240us/sample - loss: 4.1115 - accuracy: 0.1148\n",
      "Epoch 24/100\n",
      "12880/12880 [==============================] - 3s 259us/sample - loss: 4.0173 - accuracy: 0.1239\n",
      "Epoch 25/100\n",
      "12880/12880 [==============================] - 3s 259us/sample - loss: 3.9287 - accuracy: 0.1325\n",
      "Epoch 26/100\n",
      "12880/12880 [==============================] - 4s 296us/sample - loss: 3.8352 - accuracy: 0.1429\n",
      "Epoch 27/100\n",
      "12880/12880 [==============================] - 4s 284us/sample - loss: 3.7430 - accuracy: 0.1546\n",
      "Epoch 28/100\n",
      "12880/12880 [==============================] - 4s 336us/sample - loss: 3.6530 - accuracy: 0.1685\n",
      "Epoch 29/100\n",
      "12880/12880 [==============================] - 4s 298us/sample - loss: 3.5665 - accuracy: 0.1785\n",
      "Epoch 30/100\n",
      "12880/12880 [==============================] - 3s 255us/sample - loss: 3.4731 - accuracy: 0.1950\n",
      "Epoch 31/100\n",
      "12880/12880 [==============================] - 3s 264us/sample - loss: 3.3824 - accuracy: 0.2107\n",
      "Epoch 32/100\n",
      "12880/12880 [==============================] - 4s 276us/sample - loss: 3.2939 - accuracy: 0.2283\n",
      "Epoch 33/100\n",
      "12880/12880 [==============================] - 4s 286us/sample - loss: 3.2099 - accuracy: 0.2414\n",
      "Epoch 34/100\n",
      "12880/12880 [==============================] - 3s 254us/sample - loss: 3.1336 - accuracy: 0.2561\n",
      "Epoch 35/100\n",
      "12880/12880 [==============================] - 4s 289us/sample - loss: 3.0479 - accuracy: 0.2709\n",
      "Epoch 36/100\n",
      "12880/12880 [==============================] - 4s 281us/sample - loss: 2.9718 - accuracy: 0.2844\n",
      "Epoch 37/100\n",
      "12880/12880 [==============================] - 4s 292us/sample - loss: 2.8968 - accuracy: 0.2996\n",
      "Epoch 38/100\n",
      "12880/12880 [==============================] - 4s 273us/sample - loss: 2.8202 - accuracy: 0.3217\n",
      "Epoch 39/100\n",
      "12880/12880 [==============================] - 3s 268us/sample - loss: 2.7537 - accuracy: 0.3303\n",
      "Epoch 40/100\n",
      "12880/12880 [==============================] - 4s 298us/sample - loss: 2.6787 - accuracy: 0.3450\n",
      "Epoch 41/100\n",
      "12880/12880 [==============================] - 3s 244us/sample - loss: 2.6170 - accuracy: 0.3584\n",
      "Epoch 42/100\n",
      "12880/12880 [==============================] - 4s 295us/sample - loss: 2.5503 - accuracy: 0.3740\n",
      "Epoch 43/100\n",
      "12880/12880 [==============================] - 3s 263us/sample - loss: 2.4950 - accuracy: 0.3838\n",
      "Epoch 44/100\n",
      "12880/12880 [==============================] - 3s 261us/sample - loss: 2.4301 - accuracy: 0.4009\n",
      "Epoch 45/100\n",
      "12880/12880 [==============================] - 3s 249us/sample - loss: 2.3665 - accuracy: 0.4146\n",
      "Epoch 46/100\n",
      "12880/12880 [==============================] - 4s 300us/sample - loss: 2.3146 - accuracy: 0.4255\n",
      "Epoch 47/100\n",
      "12880/12880 [==============================] - 3s 256us/sample - loss: 2.2587 - accuracy: 0.4403\n",
      "Epoch 48/100\n",
      "12880/12880 [==============================] - 3s 263us/sample - loss: 2.2060 - accuracy: 0.4512\n",
      "Epoch 49/100\n",
      "12880/12880 [==============================] - 4s 290us/sample - loss: 2.1545 - accuracy: 0.4633\n",
      "Epoch 50/100\n",
      "12880/12880 [==============================] - 4s 306us/sample - loss: 2.0986 - accuracy: 0.4780\n",
      "Epoch 51/100\n",
      "12880/12880 [==============================] - 4s 272us/sample - loss: 2.0557 - accuracy: 0.4884\n",
      "Epoch 52/100\n",
      "12880/12880 [==============================] - 3s 245us/sample - loss: 2.0125 - accuracy: 0.5003\n",
      "Epoch 53/100\n",
      "12880/12880 [==============================] - 4s 272us/sample - loss: 1.9666 - accuracy: 0.5074\n",
      "Epoch 54/100\n",
      "12880/12880 [==============================] - 3s 271us/sample - loss: 1.9182 - accuracy: 0.5232\n",
      "Epoch 55/100\n",
      "12880/12880 [==============================] - 4s 288us/sample - loss: 1.8732 - accuracy: 0.5323\n",
      "Epoch 56/100\n",
      "12880/12880 [==============================] - 3s 262us/sample - loss: 1.8317 - accuracy: 0.5401\n",
      "Epoch 57/100\n",
      "12880/12880 [==============================] - 4s 304us/sample - loss: 1.7949 - accuracy: 0.5477\n",
      "Epoch 58/100\n",
      "12880/12880 [==============================] - 3s 252us/sample - loss: 1.7569 - accuracy: 0.5589\n",
      "Epoch 59/100\n",
      "12880/12880 [==============================] - 3s 252us/sample - loss: 1.7130 - accuracy: 0.5689\n",
      "Epoch 60/100\n",
      "12880/12880 [==============================] - 3s 262us/sample - loss: 1.6716 - accuracy: 0.5834\n",
      "Epoch 61/100\n",
      "12880/12880 [==============================] - 3s 262us/sample - loss: 1.6336 - accuracy: 0.5932\n",
      "Epoch 62/100\n",
      "12880/12880 [==============================] - 3s 265us/sample - loss: 1.6145 - accuracy: 0.5950\n",
      "Epoch 63/100\n",
      "12880/12880 [==============================] - 4s 306us/sample - loss: 1.5768 - accuracy: 0.6039\n",
      "Epoch 64/100\n",
      "12880/12880 [==============================] - 3s 249us/sample - loss: 1.5407 - accuracy: 0.6162\n",
      "Epoch 65/100\n",
      "12880/12880 [==============================] - 4s 283us/sample - loss: 1.5085 - accuracy: 0.6224\n",
      "Epoch 66/100\n",
      "12880/12880 [==============================] - 4s 281us/sample - loss: 1.4745 - accuracy: 0.6329\n",
      "Epoch 67/100\n",
      "12880/12880 [==============================] - 3s 226us/sample - loss: 1.4422 - accuracy: 0.6391\n",
      "Epoch 68/100\n",
      "12880/12880 [==============================] - 3s 232us/sample - loss: 1.4140 - accuracy: 0.6475\n",
      "Epoch 69/100\n",
      "12880/12880 [==============================] - 3s 233us/sample - loss: 1.3757 - accuracy: 0.6548\n",
      "Epoch 70/100\n",
      "12880/12880 [==============================] - 3s 238us/sample - loss: 1.3599 - accuracy: 0.6601\n",
      "Epoch 71/100\n",
      "12880/12880 [==============================] - 3s 225us/sample - loss: 1.3335 - accuracy: 0.6661\n",
      "Epoch 72/100\n",
      "12880/12880 [==============================] - 3s 210us/sample - loss: 1.3033 - accuracy: 0.6709\n",
      "Epoch 73/100\n",
      "12880/12880 [==============================] - 3s 224us/sample - loss: 1.2715 - accuracy: 0.6814\n",
      "Epoch 74/100\n",
      "12880/12880 [==============================] - 3s 215us/sample - loss: 1.2466 - accuracy: 0.6861\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12880/12880 [==============================] - 3s 242us/sample - loss: 1.2351 - accuracy: 0.6892\n",
      "Epoch 76/100\n",
      "12880/12880 [==============================] - 3s 240us/sample - loss: 1.1993 - accuracy: 0.6977\n",
      "Epoch 77/100\n",
      "12880/12880 [==============================] - 3s 243us/sample - loss: 1.1777 - accuracy: 0.7043\n",
      "Epoch 78/100\n",
      "12880/12880 [==============================] - 3s 232us/sample - loss: 1.1594 - accuracy: 0.7100\n",
      "Epoch 79/100\n",
      "12880/12880 [==============================] - 3s 241us/sample - loss: 1.1396 - accuracy: 0.7133\n",
      "Epoch 80/100\n",
      "12880/12880 [==============================] - 3s 237us/sample - loss: 1.1332 - accuracy: 0.7127\n",
      "Epoch 81/100\n",
      "12880/12880 [==============================] - 3s 240us/sample - loss: 1.0981 - accuracy: 0.7249\n",
      "Epoch 82/100\n",
      "12880/12880 [==============================] - 3s 229us/sample - loss: 1.0927 - accuracy: 0.7259\n",
      "Epoch 83/100\n",
      "12880/12880 [==============================] - 3s 228us/sample - loss: 1.0724 - accuracy: 0.7291\n",
      "Epoch 84/100\n",
      "12880/12880 [==============================] - 3s 237us/sample - loss: 1.0467 - accuracy: 0.7364\n",
      "Epoch 85/100\n",
      "12880/12880 [==============================] - 3s 260us/sample - loss: 1.0225 - accuracy: 0.7405\n",
      "Epoch 86/100\n",
      "12880/12880 [==============================] - 4s 279us/sample - loss: 1.0032 - accuracy: 0.7474\n",
      "Epoch 87/100\n",
      "12880/12880 [==============================] - 3s 246us/sample - loss: 0.9944 - accuracy: 0.7483\n",
      "Epoch 88/100\n",
      "12880/12880 [==============================] - 3s 243us/sample - loss: 0.9868 - accuracy: 0.7505\n",
      "Epoch 89/100\n",
      "12880/12880 [==============================] - 3s 238us/sample - loss: 0.9956 - accuracy: 0.7449\n",
      "Epoch 90/100\n",
      "12880/12880 [==============================] - 3s 250us/sample - loss: 0.9856 - accuracy: 0.7472\n",
      "Epoch 91/100\n",
      "12880/12880 [==============================] - 3s 248us/sample - loss: 0.9624 - accuracy: 0.7530\n",
      "Epoch 92/100\n",
      "12880/12880 [==============================] - 4s 276us/sample - loss: 0.9452 - accuracy: 0.7580\n",
      "Epoch 93/100\n",
      "12880/12880 [==============================] - 4s 302us/sample - loss: 0.9179 - accuracy: 0.7656\n",
      "Epoch 94/100\n",
      "12880/12880 [==============================] - 3s 252us/sample - loss: 0.8986 - accuracy: 0.7716\n",
      "Epoch 95/100\n",
      "12880/12880 [==============================] - 3s 260us/sample - loss: 0.8877 - accuracy: 0.7744\n",
      "Epoch 96/100\n",
      "12880/12880 [==============================] - 3s 243us/sample - loss: 0.8819 - accuracy: 0.7724\n",
      "Epoch 97/100\n",
      "12880/12880 [==============================] - 3s 253us/sample - loss: 0.9033 - accuracy: 0.7668\n",
      "Epoch 98/100\n",
      "12880/12880 [==============================] - 3s 249us/sample - loss: 0.9245 - accuracy: 0.7593\n",
      "Epoch 99/100\n",
      "12880/12880 [==============================] - 3s 256us/sample - loss: 0.8789 - accuracy: 0.7722\n",
      "Epoch 100/100\n",
      "12880/12880 [==============================] - 3s 225us/sample - loss: 0.8472 - accuracy: 0.7796\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo pode agora ser utilizado para  prever as próximas palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hue'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_next_word(model, frase):\n",
    "    sequences = tokenizer.texts_to_sequences([frase])\n",
    "    padded = pad_sequences(sequences, maxlen=max_len , padding='pre', truncating='pre')\n",
    "    predicted = model.predict_classes(padded)\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    return output_word\n",
    "    \n",
    "get_next_word(model, 'please, save me in')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inclusive, podemos gerar uma sequencia maior de palavras para formar um novo soneto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please, save me in this whom those swift hand decay lie me still to none alive of love of you glory go the heart go love decay show their end heart decay seen much life and end my worth heart dead live best me not go go allow treasure me not shine ever ever ever\n"
     ]
    }
   ],
   "source": [
    "generate_words = 50\n",
    "\n",
    "sentence = 'please, save me in this'\n",
    "for i in range(generate_words):\n",
    "    sentence = sentence + \" \" + get_next_word(model, sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uma determinada entrada, o modelo sempre vai responder a mesma palavra, o que não é interessante. Modifique a funcão `get_next_word` para retornar uma palavra com probabilidade igual à especificada pelo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste diversas possibilidades de arquitetura, com modelos bidirecionais, LSTM e GRUs, quais deles obtém um resultado melhor?\n",
    "\n",
    "\n",
    "Após isso, tente treinar um modelo semelhante usando poemas de [Goncalves Dias](http://www.dominiopublico.gov.br/download/texto/bv000115.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
